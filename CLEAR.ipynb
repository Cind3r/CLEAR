{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a97bcb8",
   "metadata": {},
   "source": [
    "# CLEAR\n",
    "\n",
    "Chargemaster Location-based Exploration for Affordability & Reform\n",
    "\n",
    "This is the notebook file primarily responsible for pre-processing data, attaching important information, and generating database files for the github page. Below you can find all information about how data is processed from the downloaded `.csv` files found on most hospital sites. This is an exploratory project focused on creating interactive visualzations and tools to better inform people about their healthcare. The repo can always be maintained by downloading the most current year data for the specific hospital and putting it through the scripts. It should be noted that this is NOT a comprehensive list, but it can potentially be scaled to a full working-standalone site with enough time. \n",
    "\n",
    "All pre-processing code is written in python. See the `.html` files for how the D3 visualizations work. \n",
    "\n",
    "## How it works (Copied from README)\n",
    "\n",
    "Hospitals that have been added to this 'web-app' are stored in a `.csv` file for quick look up and ease of access. This points to the loc of it's Charge Master `.json` file which is then queried for the specific procedure. Hospitals are gathered from the CSV list based on a radius look-up provided by the user. If a hospital in the radius does not offer the service, it will not display the price point compared to others in the radius. \n",
    "\n",
    "Currently limited to 500 procedures due to file size limits and me not wanting to set up a server/database for this. Parquet only works server side so i can't do iterative testing before publishing to pages, and pages deployments can take a while. Will consider moving to parquet system after front-end is stable and working as envisioned.\n",
    "\n",
    "## List of Hospitals\n",
    "\n",
    "These are the hospital's which data has been gathered and processed for thus far:\n",
    "\n",
    "| State    | Hospital Name                     | Zipcode     | Date                 | File Size    | Link                                                            |\n",
    "|----------|--------------------------------|-------------|-------------------|-------------|------------------------------------------------|\n",
    "| NC        | Duke University Hospital     |     27710    |      09/2025      |   3.32 GB   |    [Link](https://www.dukehealth.org/paying-for-care/what-duke-charges-services) |\n",
    "| NC        | AdventHealth (Hendersonville)   |     28792   |   09/2025    |      1.48 GB           |                                                                 |\n",
    "| NC | UNC Rex Hospital | 27606 | 09/2025 | 121 MB | [Link](https://www.unchealth.org/records-insurance/standard-charges) |\n",
    "| NC | WakeMed North Hospital | 27614 | 09/2025 | 56.1 MB | [Link](https://www.wakemed.org/sites/default/files/PricingTransparency/566017737_wakemed-raleigh-campus-and-north-hospital_standardcharges.csv) |\n",
    "| SC        | MUSC Health-University Medical Center (Charleston) |   29425   | 09/2025 | 11.8 MB |  [Link](https://muschealth.org/patients-visitors/billing/price-transparency) |\n",
    "| VA | Inova Fairfax Hospital (Falls Church) | 22042 | 09/2025 | 11.3 MB | [Link](https://www.inova.org/patient-and-visitor-information/hospital-charges) |\n",
    "\n",
    "#### Top Hospitals in Every State \n",
    "\n",
    "A list of hospitals that should be added at a later date.\n",
    "\n",
    "- Alaska: Providence Alaska Medical Center (Anchorage) and Fairbanks Memorial Hospital\n",
    "- Alabama: University of Alabama at Birmingham Hospital        \n",
    "- Arizona: Mayo Clinic-Phoenix        \n",
    "- Arkansas: Washington Regional Medical Center (Fayetteville)        \n",
    "- California: Cedars-Sinai Medical Center (Los Angeles), UCLA Medical Center (Los Angeles), Stanford Health Care-Stanford Hospital (Palo Alto), UC San Diego Health-LaJolla and Hillcrest Hospitals, and UCSF Health-UCSF Medical Center (San Francisco)\n",
    "- Colorado: UCHealth University of Colorado Hospital (Aurora)        \n",
    "- Connecticut: Yale New Haven Hospital        \n",
    "- Delaware: ChristianaCare Hospitals (Newark)        \n",
    "- Florida: Mayo Clinic-Jacksonville        \n",
    "- Georgia: Emory University Hospital (Atlanta)        \n",
    "- Hawaii: Queen’s Medical Center (Honolulu)        \n",
    "- Idaho: St. Luke’s Regional Medical Center (Boise)        \n",
    "- Illinois: Northwestern Medicine-Northwestern Memorial Hospital (Chicago) and Rush University Hospital (Chicago)         \n",
    "- Indiana: Indiana University Health Medical Center (Indianapolis)        \n",
    "- Iowa: University of Iowa Hospitals and Clinics (Iowa City)        \n",
    "- Kansas: University of Kansas Hospital (Kansas City)        \n",
    "- Kentucky: University of Kentucky Albert B. Chandler Hospital (Lexington) \n",
    "- Louisiana: Ochsner Medical Center (New Orleans)        \n",
    "- Maine: Maine Medical Center (Portland)        \n",
    "- Maryland: Johns Hopkins Hospital (Baltimore)        \n",
    "- Massachusetts: Massachusetts General Hospital (Boston) and Brigham and Women’s Hospital (Boston)       \n",
    "- Michigan: University of Michigan Health-Ann Arbor        \n",
    "- Minnesota: Mayo Clinic (Rochester)        \n",
    "- Mississippi: Mississippi Baptist Medical Center (Jackson)        \n",
    "- Missouri: Barnes-Jewish Hospital (St. Louis)        \n",
    "- Montana: Billings Clinic        \n",
    "- Nebraska: Nebraska Medicine-Nebraska Medical Center (Omaha)        \n",
    "- Nevada: Renown Regional Medical Center (Reno)        \n",
    "- New Hampshire: Dartmouth Hitchcock Medical Center (Lebanon)        \n",
    "- New Jersey: Hackensack University Medical Center at Hackensack University Health      \n",
    "- New Mexico: Presbyterian Hospital (Albuquerque)        \n",
    "- New York: NYU Langone Hospitals (New York City), New York-Presbyterian Hospital-Columbia and Cornell (New York City), Mount Sinai Hospital (New York City), and North - Shore University Hospital at Northwell Health (Manhasset)        \n",
    "- ~~North Carolina: Duke University Hospital (Durham)~~       \n",
    "- North Dakota: Sanford Medical Center Fargo        \n",
    "- Ohio: Cleveland Clinic        \n",
    "- Oklahoma: St. Francis Hospital-Tulsa        \n",
    "- Oregon: OHSU Hospital (Portland)        \n",
    "- Pennsylvania: Hospitals of the University of Pennsylvania-Penn Presbyterian (Philadelphia)        \n",
    "- Rhode Island: Miriam Hospital (Providence)        \n",
    "- ~~South Carolina: MUSC Health-University Medical Center (Charleston)~~        \n",
    "- South Dakota: Sanford USD Medical Center (Sioux Falls)        \n",
    "- Tennessee: Vanderbilt University Medical Center (Nashville)        \n",
    "- Texas: Houston Methodist Hospital and UT Southwestern Medical Center (Dallas)\n",
    "- Utah: University of Utah Hospital (Salt Lake City)        \n",
    "- Vermont: University of Vermont Medical Center (Burlington)        \n",
    "- ~~Virginia: Inova Fairfax Hospital (Falls Church)~~        \n",
    "- Washington: UW Medicine-University of Washington Medical Center (Seattle)        \n",
    "- West Virginia: West Virginia University Hospitals (Morgantown)        \n",
    "- Wisconsin: UW Health University Hospital (Madison)\n",
    "\n",
    "As by Becker https://www.beckershospitalreview.com/rankings-and-ratings/us-news-top-hospitals-by-state-for-2023-24/\n",
    "\n",
    "## Outside Sources Used\n",
    "\n",
    "- zip_centroids.csv courtesy of SimpleMaps data https://simplemaps.com/data/us-zips.\n",
    "- CMS.gov data \n",
    "    - for top 200 HCPCS and CPT codes billed for 2024 & top 100 lab codes. [Link](https://www.cms.gov/data-research/statistics-trends-and-reports/medicare-fee-for-service-parts-a-b/medicare-utilization-part-b)\n",
    "    - PFALL25 \n",
    "    - PFS (Physician scheduling fee) for mapping HCPCS/CPT codes to medicare rates [Link](https://www.cms.gov/medicare/payment/fee-schedules/physician/national-payment-amount-file)\n",
    "    - ASC Rates for mapping HCPCS/CPT codes to ambulatory rates [Link](https://www.cms.gov/medicare/payment/prospective-payment-systems/ambulatory-surgical-center-asc/asc-payment-rates-addenda)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## MAJOR CHANGES\n",
    "\n",
    "- Moved all HCPCS/CPT top 200 & lab codes into a single file\n",
    "- removed depreciated codes from 2024 to 2025 since we're working with 2025 CMs\n",
    "- pulled CPT code descriptions from CMS RVU data for early 2025 (jan)\n",
    "- validated 2024 codes against 2025 to ensure all are being caught properly\n",
    "- update bundler to reflect this\n",
    "\n",
    "data processing for hospitals will need to be updated to reflect these changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bdabc1",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "CSV files are too large to store on github, thus they are downloaded locally, converted to the necessary format, then uploaded. If you want to perform conversions yourself you will need to find the specific hospital chargemaster and document in the notebook accordingly.\n",
    "\n",
    "Not all Charge Masters (CM) are formatted the same, as such, to keep this notebook from growing too large, custom python scripts will be made for unique CM's. This matters beccause some hospitals are regional or statewide 'chains' but can vary prices between locations. For example, \n",
    "\n",
    "**AdventHealth**\n",
    "- AdventHealth Orlando\n",
    "- AdventHealth Tampa\n",
    "- AdventHealth Hendersonville\n",
    "\n",
    "all are AdventHealth hospitals, but their prices and available procedures vary per location. However, the same script to clean and process their CM's works because the file structure doesn't change from loc to loc. Normally CM structure only changes from hospital to hospital (brand-wise), but I haven't looked at the majority of US hospitals so this statement might need to be amended. \n",
    "\n",
    "Think of this file as more of a \"**Controller**\" for the cleaning, while the cleaning process is performed by imported functions. Subsections from here on are labeled by State, be sure to check which Hospitals are in each subsection before uploading data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bf0c7",
   "metadata": {},
   "source": [
    "***\n",
    "## Payer & Plan Names\n",
    "\n",
    "Naming conventions for payer/plans differ across hospitals, making this a pain. Like is an exhaustive regex section to hopefully simplify this so that the functionality of the .html page remains. \n",
    "\n",
    "Idk where this fits in, I'll add it later to documentation.\n",
    "\n",
    "use this to create a comprehensive list of all current payer names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ce11e",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Preloading Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospitals.csv updater/editor\n",
    "import hashlib\n",
    "import requests\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import nbformat\n",
    "from scripts.cleaners import apply_payer_standardization_to_json, standardize_payer_name\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"CLEAR-geoapi-2025\")\n",
    "csv_file = 'docs/data/hospitals.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# construct address for geocoding only (don't modify original data)\n",
    "def construct_geocoding_address(row):\n",
    "    # Build clean address from original components\n",
    "    address = f\"{row['address']}, {row['city']}, {row['state']} {row['zip']}\"\n",
    "    return address\n",
    "\n",
    "# get lat/lon from address with increased timeout and retry/delay\n",
    "def get_lat_lon(address, max_retries=3, delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            location = geolocator.geocode(address, timeout=5)\n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "            else:\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding {address} (attempt {attempt+1}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None, None\n",
    "\n",
    "# generate short unique ID based on ['hospital'] + full composite address (base36, 8 chars)\n",
    "def generate_short_id(row):\n",
    "    full_address = construct_geocoding_address(row)\n",
    "    unique_string = f\"{row['name']}_{full_address}\"\n",
    "    hash_int = int(hashlib.md5(unique_string.encode()).hexdigest(), 16)\n",
    "    short_id = base36encode(hash_int)[:8]\n",
    "    return short_id\n",
    "\n",
    "# base36 encoding for shorter IDs\n",
    "def base36encode(number):\n",
    "    chars = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    if number == 0:\n",
    "        return '0'\n",
    "    result = ''\n",
    "    while number > 0:\n",
    "        number, i = divmod(number, 36)\n",
    "        result = chars[i] + result\n",
    "    return result\n",
    "\n",
    "# Add lat/lon and short_id to dataframe, set json_path to be '/data/prices/['state']/['id'].json'\n",
    "def update_dataframe(df):\n",
    "    \n",
    "    # Don't modify the address column - just use it for geocoding\n",
    "    def lat_lon_with_delay(row):\n",
    "        geocoding_address = construct_geocoding_address(row)\n",
    "        lat, lon = get_lat_lon(geocoding_address)\n",
    "        time.sleep(1)  # 1 second delay per request\n",
    "        return pd.Series([lat, lon])\n",
    "    \n",
    "    df[['lat', 'lon']] = df.apply(lat_lon_with_delay, axis=1)\n",
    "    df['id'] = df.apply(generate_short_id, axis=1)\n",
    "    df['json_path'] = df.apply(lambda row: f\"docs/data/prices/{row['state']}/{row['id']}.json\", axis=1)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "update_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to create comparison df's for the top 200 HCPCS and CMS codes billed for 2024 & top 100 lab codes\n",
    "# first load the codes from the .csv files\n",
    "hcpcs_codes = pd.read_csv('docs/data/hcpcs_lvl2_top_200_codes_2024.csv')\n",
    "lab_codes = pd.read_csv('docs/data/lab_top_100_codes_2024.csv')\n",
    "cpt_codes = pd.read_csv('docs/data/cpt_lvl1_top_200_codes_2024.csv')\n",
    "\n",
    "# new master file for all codes\n",
    "all_codes = pd.read_csv('docs/data/top_codes_master_dictionary_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TO LOAD HOSPITALS CSV\n",
    "import os\n",
    "\n",
    "# csv's are stored locally outside of CLEAR repo\n",
    "# set up one folder then into 'ChargeMaster_Project/csv_files/'\n",
    "# get path to csv_files folder outside CLEAR repo\n",
    "workspace_root = os.path.dirname(os.path.abspath('CLEAR.ipynb'))\n",
    "csv_folder = os.path.join(workspace_root, '..', 'ChargeMaster_Project', 'csv_files')\n",
    "csv_folder = os.path.abspath(csv_folder)\n",
    "\n",
    "# define path to hospitals.csv\n",
    "hospitals_csv = os.path.join(workspace_root, 'docs', 'data', 'hospitals.csv')\n",
    "hospitals_csv = os.path.abspath(hospitals_csv)\n",
    "\n",
    "# read hospitals.csv to get list of hospitals and their file paths\n",
    "hospitals_df = pd.read_csv(hospitals_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finish implementation later and move\n",
    "def update_hospital_table():\n",
    "    # Update the first markdown cell's hospital table with new hospitals from hospitals.csv\n",
    "    notebook_path = 'CLEAR.ipynb'\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "    # Find the first markdown cell with the hospital table\n",
    "    table_md_header = \"| State    | Hospital Name\"\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown' and table_md_header in cell.source:\n",
    "            lines = cell.source.splitlines()\n",
    "            # Find start and end of the table\n",
    "            table_start = next(i for i, l in enumerate(lines) if l.strip().startswith(table_md_header))\n",
    "            table_rows = lines[table_start+2:]  # skip header and separator\n",
    "            existing_names = set()\n",
    "            for row in table_rows:\n",
    "                parts = row.split('|')\n",
    "                if len(parts) > 2:\n",
    "                    existing_names.add(parts[2].strip())\n",
    "            # Add new hospitals not already in the table\n",
    "            new_rows = []\n",
    "            for _, row in hospitals_df.iterrows():\n",
    "                if row['name'] not in existing_names:\n",
    "                    new_rows.append(f\"| {row['state']} | {row['name']} | {row['zip']} |  |  |\")\n",
    "            # Insert new rows after the last table row\n",
    "            updated_lines = lines[:table_start+2] + table_rows + new_rows\n",
    "            cell.source = \"\\n\".join(updated_lines)\n",
    "            break\n",
    "\n",
    "    # Save the updated notebook\n",
    "    with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208356c9",
   "metadata": {},
   "source": [
    "***\n",
    "## North Carolina Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcing\\AppData\\Local\\Temp\\ipykernel_5708\\1631332753.py:19: DtypeWarning: Columns (7,8,9,12,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  duke_df = pd.read_csv(test_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duke Hospital test processing complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================================\n",
    "# --------------- DUKE HOSPITAL TESTING ----------------\n",
    "# ======================================================================\n",
    "\n",
    "# Grab row for Duke Hospital in Durham, NC\n",
    "hos_name = 'Duke University Hospital'\n",
    "matching_hospitals = hospitals_df[hospitals_df['name'] == hos_name]\n",
    "if not matching_hospitals.empty:\n",
    "    duke_row = matching_hospitals.iloc[0]\n",
    "else:\n",
    "    print(f\"Hospital '{hos_name}' not found in the dataset\")\n",
    "    duke_row = None\n",
    "\n",
    "# grab json path for Duke Hospital\n",
    "duke_json_path = duke_row['json_path']\n",
    "\n",
    "# load a single csv file from csv_folder for testing\n",
    "test_csv_path = os.path.join(csv_folder, 'DukeHospital_Durham.csv')\n",
    "duke_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Change all mixed type columns to string to avoid dtype issues\n",
    "for col in duke_df.columns:\n",
    "    if duke_df[col].dtype == 'object':\n",
    "        duke_df[col] = duke_df[col].astype(str)\n",
    "\n",
    "\n",
    "# remove duke_df Hospital, City, State, Address columns before converting to parquet\n",
    "duke_df = duke_df.drop(columns=['Hospital', 'City', 'State', 'Address'])\n",
    "\n",
    "\n",
    "#code_cols = ['code_1', 'code_2', 'code_3', 'code_4']\n",
    "# Check matches for each code column against hcpcs_codes, cpt_codes, and lab_codes, iteratively\n",
    "# for col in code_cols:\n",
    "#     print(f\"Checking matches for column: {col}\")\n",
    "#     hcpcs_matches = duke_df[duke_df[col].isin(hcpcs_codes['HCPCS Code'])]\n",
    "#     cpt_matches = duke_df[duke_df[col].isin(cpt_codes['HCPCS Code'])]\n",
    "#     lab_matches = duke_df[duke_df[col].isin(lab_codes['HCPCS Code'])]\n",
    "#     print(f\"  HCPCS matches: {len(hcpcs_matches)}\")\n",
    "#     print(f\"  CPT matches: {len(cpt_matches)}\")\n",
    "#     print(f\"  Lab matches: {len(lab_matches)}\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    This actually shows that code_2 contains HCPCS codes and code_3 contains CPT codes\n",
    "    Checking matches for column: code_1\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 0\n",
    "        Lab matches: 0\n",
    "    Checking matches for column: code_2\n",
    "        HCPCS matches: 76966\n",
    "        CPT matches: 0\n",
    "        Lab matches: 19987\n",
    "    Checking matches for column: code_3\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 772\n",
    "        Lab matches: 0\n",
    "    Checking matches for column: code_4\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 0\n",
    "        Lab matches: 0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Duke Hospital CM Structure\n",
    "# code_2/code_3 [columns 3, 5 --> 4, 6 contain type] contain HCPCS and CPT codes, so we use those for comparison against the top 200 lists\n",
    "# Columns 13-24 contain payer, plan, and pricing info, so we want all of those as well as column 0 which is the \n",
    "# description of the code [used for regex matching on the front end]\n",
    "# final columns to keep: 0, 3-6, 13-24\n",
    "duke_df = duke_df.iloc[:, [0] + list(range(3, 7)) + list(range(13, 25))]\n",
    "\n",
    "# actually lets go ahead and drop some columns to conserve space\n",
    "duke_df = duke_df.drop(columns=['standard_charge_algorithm', 'additional_generic_notes'])\n",
    "\n",
    "# now we can search duke_df['code_2'] and duke_df['code_2_type'] against hcpcs_codes , cpt_codes, and lab_codes\n",
    "# first search hcpcs_codes\n",
    "hcpcs_matches = duke_df[duke_df['code_2'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = duke_df[duke_df['code_3'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = duke_df[duke_df['code_2'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "\n",
    "# Apply standardization to all_matches if not empty\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "    all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "    # redrop possible duplicates\n",
    "    all_matches = all_matches.drop_duplicates()\n",
    "else:\n",
    "    # Create empty DataFrame with same structure as duke_df if no matches\n",
    "    all_matches = pd.DataFrame(columns=duke_df.columns)\n",
    "\n",
    "# There are some duplicate issues, mainly rows where no est. price are given, so lets remove enteries that don't have est. prices\n",
    "all_matches = all_matches[all_matches['estimated_amount'].notna() & (all_matches['estimated_amount'] != '')]\n",
    "\n",
    "# Now we need to combine the columns, code_2 is generally more important so we save that over 3 if they're both present\n",
    "# Combine code_2/code_2_type and code_3/code_3_type into 'code' and 'type'\n",
    "def select_code(row):\n",
    "    if pd.notna(row['code_2']) and row['code_2'] != '':\n",
    "        return pd.Series({'code': row['code_2'], 'type': row['code_2_type']})\n",
    "    elif pd.notna(row['code_3']) and row['code_3'] != '':\n",
    "        return pd.Series({'code': row['code_3'], 'type': row['code_3_type']})\n",
    "    else:\n",
    "        return pd.Series({'code': None, 'type': None})\n",
    "\n",
    "all_matches[['code', 'type']] = all_matches.apply(select_code, axis=1)\n",
    "all_matches = all_matches.drop(columns=['code_2', 'code_2_type', 'code_3', 'code_3_type'])\n",
    "all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "# Save output data to json file for Duke json path\n",
    "all_matches.to_json(duke_json_path, orient='records', lines=True)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del duke_df\n",
    "del duke_row\n",
    "del test_csv_path\n",
    "del all_matches\n",
    "\n",
    "print(\"Duke Hospital test processing complete.\")\n",
    "# ======================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcing\\AppData\\Local\\Temp\\ipykernel_5708\\1743173882.py:27: DtypeWarning: Columns (1,2,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  adv_nc_df = pd.read_csv(adv_nc_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdventHealth Hendersonville, NC test processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# --------------- ADVENTHEALTH HOSPITAL  ----------------\n",
    "# ======================================================================\n",
    "\n",
    "# Load AdventHealth Hendersonville, NC paths\n",
    "hos_name = 'AdventHealth'\n",
    "city_name = 'Hendersonville'\n",
    "state_name = 'NC'\n",
    "matching_hospitals = hospitals_df[\n",
    "    (hospitals_df['name'] == hos_name) &\n",
    "    (hospitals_df['state'] == state_name) &\n",
    "    (hospitals_df['city'] == city_name)\n",
    "]\n",
    "if not matching_hospitals.empty:\n",
    "    adv_nc = matching_hospitals.iloc[0]\n",
    "else:\n",
    "    print(f\"Hospital '{hos_name}' not found in the dataset\")\n",
    "    adv_nc = None\n",
    "\n",
    "# grab json path for AdventHealth Hendersonville, NC\n",
    "adv_nc_json_path = adv_nc['json_path']\n",
    "\n",
    "# load a single csv file from csv_folder for testing\n",
    "adv_nc_csv_path = os.path.join(csv_folder, 'AdventHealth_Hendersonville_CM.csv')\n",
    "\n",
    "# load AdventHealth Hendersonville, NC csv\n",
    "adv_nc_df = pd.read_csv(adv_nc_csv_path)\n",
    "\n",
    "# AdventHealth CM Structure\n",
    "# ['description', 'drug_information', 'code', 'type',\n",
    "#    'standard_charge_min', 'standard_charge_max', 'gross_charge',\n",
    "#    'discounted_cash', 'setting', 'payer_name', 'plan_name',\n",
    "#    'standard_charge_dollar', 'standard_charge_percentage',\n",
    "#    'estimated_amount', 'methodology', 'standard_charge_algorithm',\n",
    "#    'Hospital', 'City', 'State', 'Address']\n",
    "\n",
    "# Check matches for code column against hcpcs_codes, cpt_codes, and lab_codes\n",
    "# Output: \n",
    "# HCPCS matches: 20845\n",
    "#   CPT matches: 2884\n",
    "#   Lab matches: 13555\n",
    "\n",
    "# Lets drop unneeded columns, and rename some before grabing the matches and saving to json\n",
    "# NOTE: common naming convention needs to be added before renaming cols\n",
    "cols_to_drop = ['methodology', 'drug_information', 'standard_charge_algorithm', 'Hospital', 'City', 'State', 'Address']\n",
    "adv_nc_df = adv_nc_df.drop(columns=cols_to_drop)\n",
    "\n",
    "# now grab matches\n",
    "hcpcs_matches = adv_nc_df[adv_nc_df['code'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = adv_nc_df[adv_nc_df['code'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = adv_nc_df[adv_nc_df['code'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "    all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "    # redrop possible duplicates\n",
    "    all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "else:\n",
    "    # Create empty DataFrame with same structure as duke_df if no matches\n",
    "    all_matches = pd.DataFrame(columns=duke_df.columns)\n",
    "\n",
    "# There are some duplicate issues, mainly rows where no est. price are given, so lets remove enteries that don't have est. prices\n",
    "all_matches = all_matches[all_matches['estimated_amount'].notna() & (all_matches['estimated_amount'] != '')]\n",
    "\n",
    "# Save output data to json file for AdventHealth Hendersonville, NC json path\n",
    "all_matches.to_json(adv_nc_json_path, orient='records', lines=True)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del adv_nc_df\n",
    "del adv_nc\n",
    "del adv_nc_csv_path\n",
    "del all_matches\n",
    "\n",
    "print(\"AdventHealth Hendersonville, NC test processing complete.\")\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming UNC Rex data from wide to long format...\n",
      "Original shape: (160860, 215)\n",
      "Transformed shape: (1057313, 24)\n",
      "Original shape: (160860, 215)\n",
      "Transformed shape: (1057313, 24)\n",
      "Matched 8312 / 8503 rows.\n",
      "service_config.json NOT updated (no misses requiring changes).\n",
      "Skipped 191 non-plausible codes (unique 2). Examples: 731.0, 790.0\n",
      "UNC Rex Hospital test processing complete.\n",
      "Matched 8312 / 8503 rows.\n",
      "service_config.json NOT updated (no misses requiring changes).\n",
      "Skipped 191 non-plausible codes (unique 2). Examples: 731.0, 790.0\n",
      "UNC Rex Hospital test processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# ---------------  UNC REX HOSPITAL  ----------------\n",
    "# ======================================================================\n",
    "# Lets start adding info to the CSV in python rather than manual edits each time\n",
    "# Load UNC Rex Hospital in Raleigh, NC paths\n",
    "from scripts.cleaners import transform_wide_to_long_format\n",
    "from scripts.bundle_validation import ValidateJSON\n",
    "\n",
    "hos_name = 'UNC Rex Hospital'\n",
    "\n",
    "# add hospital info for UNC Rex to hospitals.csv\n",
    "address = '4420 Lake Boone Trail'\n",
    "city_name = 'Raleigh'\n",
    "state_name = 'NC'\n",
    "zip_code = '27607'\n",
    "\n",
    "# update hospitals_df with new entry if it doesn't already exist\n",
    "# move this to be a function later\n",
    "if hospitals_df[\n",
    "    (hospitals_df['name'] == hos_name) &\n",
    "    (hospitals_df['state'] == state_name) &\n",
    "    (hospitals_df['city'] == city_name)\n",
    "].empty:\n",
    "    new_entry = {\n",
    "        'name': hos_name,\n",
    "        'address': address,\n",
    "        'city': city_name,\n",
    "        'state': state_name,\n",
    "        'zip': zip_code\n",
    "    }\n",
    "    hospitals_df = pd.concat([hospitals_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
    "    hospitals_df.to_csv(hospitals_csv, index=False)  # Save updated CSV\n",
    "    print(f\"Added new hospital entry for '{hos_name}' to hospitals.csv\")\n",
    "\n",
    "    # now run the update_dataframe function to add lat/lon, id, and json_path\n",
    "    update_dataframe(hospitals_df)\n",
    "\n",
    "# now lets grab all the paths\n",
    "unc_rex_json_path = hospitals_df[\n",
    "    (hospitals_df['name'] == hos_name) & \n",
    "    (hospitals_df['state'] == state_name) & \n",
    "    (hospitals_df['city'] == city_name)\n",
    "].iloc[0]['json_path']\n",
    "unc_rex_csv_path = os.path.join(csv_folder, 'UNCREX_CM.csv')\n",
    "\n",
    "# load UNC Rex Hospital csv\n",
    "unc_rex_df = pd.read_csv(unc_rex_csv_path)\n",
    "\n",
    "# UNC Rex CM Structure: COLUMNS -->\n",
    "# description, code|1, code|1|type, code|2, code|2|type, code|3, code|3|type, billing_class, setting,\n",
    "# drug_unit_of_measurement, drug_type_of_measurement, modifiers, standard_charge|gross,\n",
    "# standard_charge|discounted_cash, standard_charge|min, standard_charge|max, additional_generic_notes,\n",
    "# standard_charge|AETNA|CHOICE POS|negotiated_dollar, standard_charge|AETNA|CHOICE\n",
    "# POS|negotiated_percentage, standard_charge|AETNA|CHOICE POS|negotiated_algorithm,\n",
    "# standard_charge|AETNA|CHOICE POS|methodology, estimated_amount|AETNA|CHOICE POS,\n",
    "# additional_payer_notes|AETNA|CHOICE POS, ... etc for other payers/plans\n",
    "\n",
    "# here you can see we have a new type of CM structure where rather than tons of row enteries for each code/payer/plan\n",
    "# we have one row per code with multiple columns for each payer/plan combination, so we need to alter our approach to\n",
    "# processing the data\n",
    "\n",
    "# Apply the transformation\n",
    "print(\"Transforming UNC Rex data from wide to long format...\")\n",
    "unc_rex_transformed = transform_wide_to_long_format(unc_rex_df)\n",
    "\n",
    "print(f\"Original shape: {unc_rex_df.shape}\")\n",
    "print(f\"Transformed shape: {unc_rex_transformed.shape}\")\n",
    "\n",
    "# Great now we can start searching the codes against our top 200 lists\n",
    "# now grab matches\n",
    "\n",
    "\"\"\"\n",
    "Column code_1: HCPCS matches: 0, CPT matches: 128, Lab matches: 0\n",
    "Column code_2: HCPCS matches: 2746, CPT matches: 0, Lab matches: 3719\n",
    "Column code_3: HCPCS matches: 0, CPT matches: 0, Lab matches: 0\n",
    "\"\"\"\n",
    "unc_rex_transformed.drop(['code_3', 'code_3_type'], axis=1, inplace=True)  # drop unused code_3 columns\n",
    "\n",
    "# lets rename the code being grabbed to be code, and type accordingly during the match process\n",
    "# HCPCS matches: filter on code_2, set code='code_2', type='code_2_type'\n",
    "hcpcs_matches = unc_rex_transformed[unc_rex_transformed['code_2'].isin(hcpcs_codes['HCPCS Code'])].copy()\n",
    "hcpcs_matches['code'] = hcpcs_matches['code_2']\n",
    "hcpcs_matches['type'] = hcpcs_matches['code_2_type']\n",
    "hcpcs_matches = hcpcs_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# CPT matches: filter on code_1, set code='code_1', type='code_1_type'\n",
    "cpt_matches = unc_rex_transformed[unc_rex_transformed['code_1'].isin(cpt_codes['HCPCS Code'])].copy()\n",
    "cpt_matches['code'] = cpt_matches['code_1']\n",
    "cpt_matches['type'] = cpt_matches['code_1_type']\n",
    "cpt_matches = cpt_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# Lab matches: filter on code_2, set code='code_2', type='code_2_type'\n",
    "lab_matches = unc_rex_transformed[unc_rex_transformed['code_2'].isin(lab_codes['HCPCS Code'])].copy()\n",
    "lab_matches['code'] = lab_matches['code_2']\n",
    "lab_matches['type'] = lab_matches['code_2_type']\n",
    "lab_matches = lab_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "    all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "    # redrop possible duplicates\n",
    "    all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "# now lets drop some more columns to save space\n",
    "cols_to_drop = ['billing_class', 'drug_unit_of_measurement', 'drug_type_of_measurement', \n",
    "                'modifiers', 'standard_charge_algorithm', 'additional_generic_notes', 'methodology']\n",
    "all_matches = all_matches.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "# Now we can save to json\n",
    "all_matches.to_json(unc_rex_json_path, orient='records', lines=True)\n",
    "\n",
    "# Lets validate the json file \n",
    "validator = ValidateJSON(unc_rex_json_path)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del unc_rex_df\n",
    "del unc_rex_transformed\n",
    "del all_matches\n",
    "del validator\n",
    "del unc_rex_csv_path\n",
    "del unc_rex_json_path\n",
    "\n",
    "print(\"UNC Rex Hospital test processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f94de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcing\\AppData\\Local\\Temp\\ipykernel_15176\\3100850403.py:31: DtypeWarning: Columns (1,3,4,6,7,8,12,13,18,23,24,29,30,35,36,41,47,48,53,54,59,65,66,71,72,77,78,83,84,89,90,95,96,101,102,107,108,113,114,119,120,125,131,132,137,138,143,149,150,155,161,167,173,179,180,185,186,191,192,197,198,203,204,209,210) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  wake_med_df = pd.read_csv(wake_med_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: 104112 rows × 211 columns\n",
      "Available base columns: 19 out of 20 possible\n",
      "Found 160 payer-specific columns\n",
      "Found 32 unique payer/plan combinations\n",
      "Columns being dropped: 181\n",
      "Expected columns in result: 30\n",
      "Final DataFrame shape: 681680 rows × 26 columns\n",
      "Row multiplication factor: 6.55x (from 104112 to 681680)\n",
      "Actual columns dropped: 185\n",
      "Data efficiency: 0.81 (final data points / original data points)\n",
      "Original total data points: 21,967,632\n",
      "Final total data points: 17,723,680\n",
      "Loaded the following code sets:\n",
      "HCPCS Codes: 200 entries\n",
      "Lab Codes: 100 entries\n",
      "CPT Codes: 200 entries\n",
      "All Codes: 469 entries\n",
      "Identified code columns: ['code_1', 'code_2', 'code_3', 'code_4']\n",
      "Column 'code_1': Found 1773 HCPCS matches, 940 Lab matches, 770 CPT matches.\n",
      "Reassigning code_1 to 'code' and 'code_1_type' to 'type' for non-empty matches DataFrames.\n",
      "Column 'code_1': Combined matches shape: (3483, 28)\n",
      "Column 'code_1': Unique matches after dropping duplicates: 3154\n",
      "Column 'code_2': Found 9384 HCPCS matches, 3248 Lab matches, 0 CPT matches.\n",
      "Reassigning code_2 to 'code' and 'code_2_type' to 'type' for non-empty matches DataFrames.\n",
      "Column 'code_2': Combined matches shape: (12632, 28)\n",
      "Column 'code_2': Unique matches after dropping duplicates: 3280\n",
      "Column 'code_3': Found 0 HCPCS matches, 0 Lab matches, 32 CPT matches.\n",
      "Reassigning code_3 to 'code' and 'code_3_type' to 'type' for non-empty matches DataFrames.\n",
      "Column 'code_3': Combined matches shape: (32, 28)\n",
      "Column 'code_3': Unique matches after dropping duplicates: 32\n",
      "Column 'code_4': Found 0 HCPCS matches, 0 Lab matches, 0 CPT matches.\n",
      "No matches found in column 'code_4'.\n",
      "Dropped code/type columns: ['code_1', 'code_1_type', 'code_2', 'code_2_type', 'code_3', 'code_3_type', 'code_4', 'code_4_type']\n",
      "Merged DataFrame shape before dropping duplicates: (6466, 20)\n",
      "Merged DataFrame shape after dropping duplicates: (6466, 20)\n",
      "No NaN values found in 'payer_name' column...\n",
      "Total unique matched codes: 389\n",
      "Matched DataFrame shape: (6466, 20)\n",
      "Dropped unnecessary columns: ['billing_class', 'drug_unit_of_measurement', 'drug_type_of_measurement', 'modifiers', 'standard_charge_algorithm', 'additional_generic_notes', 'methodology']\n",
      "Final matched DataFrame shape: (6466, 13)\n",
      "WakeMed North Hospital test processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# ---------------  WAKE MED NORTH HOSPITAL  ----------------\n",
    "# ======================================================================\n",
    "\n",
    "# New added helper functions should reduce code bloat and need to rerun the initial cells\n",
    "import os\n",
    "import pandas as pd\n",
    "from scripts.cleaners import transform_wide_to_long_format\n",
    "from scripts.helpers import add_hospital_entry\n",
    "from scripts.code_matcher import get_matches\n",
    "from scripts.cleaners import standardize_payer_name\n",
    "\n",
    "workspace_root = os.path.dirname(os.path.abspath('CLEAR.ipynb'))\n",
    "csv_folder = os.path.join(workspace_root, '..', 'ChargeMaster_Project', 'csv_files')\n",
    "csv_folder = os.path.abspath(csv_folder)\n",
    "\n",
    "new_hos_dict = {\n",
    "    'hospital_name': 'WakeMed North Hospital',\n",
    "    'city_name': 'Raleigh',\n",
    "    'state_name': 'NC',\n",
    "    'address': '10000 Falls of Neuse Rd',\n",
    "    'zip_code': '27614',\n",
    "}\n",
    "# add hospital info for WakeMed North Hospital to hospitals.csv if it doesn't already exist\n",
    "hospitals_df = add_hospital_entry(new_hos_dict)\n",
    "\n",
    "# now lets grab all the paths\n",
    "wake_med_json_path = hospitals_df[hospitals_df['name'] == new_hos_dict['hospital_name']]['json_path'].values[0]\n",
    "wake_med_csv_path = os.path.join(csv_folder, 'WAKEMED_NORTH_CM.csv')\n",
    "\n",
    "# load WakeMed North Hospital csv\n",
    "wake_med_df = pd.read_csv(wake_med_csv_path)\n",
    "\n",
    "# WakeMed North CM Structure is in wide format, need to transform to long format first\n",
    "wake_med_long_df = transform_wide_to_long_format(wake_med_df, verbose=True)\n",
    "\n",
    "# Now grab matches using the new get_matches function\n",
    "# NOTE: WakeMed North has next to no estimated_amount data\n",
    "all_matches = get_matches(wake_med_long_df, verbose=True)\n",
    "\n",
    "# Now apply the payer standardization\n",
    "all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "# Now we can save to json\n",
    "all_matches.to_json(wake_med_json_path, orient='records', lines=True)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del wake_med_df\n",
    "del wake_med_long_df\n",
    "del all_matches\n",
    "del wake_med_csv_path\n",
    "del wake_med_json_path\n",
    "del new_hos_dict\n",
    "del hospitals_df\n",
    "\n",
    "print(\"WakeMed North Hospital test processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6969fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking matches for column: code_1\n",
      "  HCPCS matches: 1773\n",
      "  CPT matches: 770\n",
      "  Lab matches: 940\n",
      "Checking matches for column: code_2\n",
      "  HCPCS matches: 9384\n",
      "  CPT matches: 0\n",
      "  Lab matches: 3248\n",
      "Checking matches for column: code_3\n",
      "  HCPCS matches: 0\n",
      "  CPT matches: 32\n",
      "  Lab matches: 0\n",
      "Checking matches for column: code_4\n",
      "  HCPCS matches: 0\n",
      "  CPT matches: 0\n",
      "  Lab matches: 0\n"
     ]
    }
   ],
   "source": [
    "# test for matches\n",
    "codes = ['code_1', 'code_2', 'code_3', 'code_4']\n",
    "\n",
    "for col in codes:\n",
    "    print(f\"Checking matches for column: {col}\")\n",
    "    hcpcs_matches = wake_med_long_df[wake_med_long_df[col].isin(hcpcs_codes['HCPCS Code'])]\n",
    "    cpt_matches = wake_med_long_df[wake_med_long_df[col].isin(cpt_codes['HCPCS Code'])]\n",
    "    lab_matches = wake_med_long_df[wake_med_long_df[col].isin(lab_codes['HCPCS Code'])]\n",
    "    print(f\"  HCPCS matches: {len(hcpcs_matches)}\")\n",
    "    print(f\"  CPT matches: {len(cpt_matches)}\")\n",
    "    print(f\"  Lab matches: {len(lab_matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fd532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39044"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wake_med_long_df['estimated_amount'].notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2fd93e",
   "metadata": {},
   "source": [
    "***\n",
    "## South Carolina Hospitals\n",
    "\n",
    "**OH BOY AM I PISSED ALREADY**\n",
    "\n",
    "#### What’s Happening\n",
    "\n",
    "* **APC codes (`code|1` with type = APC):**\n",
    "  These rows have payer names, plan names, and estimated amounts. That’s why they’re the only rows showing price info. APC = Ambulatory Payment Classification, a CMS grouping for outpatient procedures.\n",
    "\n",
    "* **HCPCS/CPT codes (in `code|2`, `code|3`, etc. with type = CPT/HCPCS):**\n",
    "  These rows often have no payer, plan, or estimated amount attached. Instead, they are mapped *into* the APC buckets, which then carry the pricing/plan info.\n",
    "\n",
    "* In other words: the hospital publishes payer-specific negotiated rates only at the APC level, while keeping CPT/HCPCS rows as “mappings” without dollar amounts.\n",
    "\n",
    "#### How We need to Work Around It\n",
    "\n",
    "1. **Build a crosswalk (mapping):**\n",
    "\n",
    "   * Use the `description` and `code|n` columns to connect CPT/HCPCS rows to their parent APC row (same description or grouping).\n",
    "   * Then join those CPT/HCPCS codes to the APC rows that actually carry pricing.\n",
    "     → This gives you a lookup where searching by CPT/HCPCS leads you to the APC (and thus the estimated amounts and plan names).\n",
    "\n",
    "2. **Validate mapping:**\n",
    "\n",
    "   * In practice, hospitals often list the CPT/HCPCS that roll up into each APC.\n",
    "   * You’ll need to check whether identical descriptions (e.g., “Inj, aflibercept hd, 1 mg”) appear across APC-coded and CPT-coded rows, and merge them.\n",
    "\n",
    "3. **Practical solution in analysis:**\n",
    "\n",
    "   * Search by CPT -> Find matching description -> Get its APC -> Pull plan names and estimated amounts from that APC row.\n",
    "\n",
    "#### On Legality\n",
    "\n",
    "\n",
    "  Not necessarily illegal. CMS’s **price transparency rule (2021–)** requires hospitals to publish:\n",
    "\n",
    "  * Gross charges\n",
    "  * Discounted cash prices\n",
    "  * Payer-specific negotiated charges\n",
    "  * De-identified min/max negotiated charges\n",
    "  * For at least 300 shoppable services (including CPT/HCPCS).\n",
    "\n",
    "Many hospitals comply only at the APC level (grouping multiple CPTs). This practice has been criticized as undermining the intent of transparency, but hospitals often argue it’s compliant because APCs are “billing codes.” Enforcement has been light, though CMS has fined some hospitals for noncompliance.\n",
    "\n",
    "## Addendums\n",
    "\n",
    "Link to addendums for crosswalking https://www.cms.gov/medicare/payment/prospective-payment-systems/hospital-outpatient-pps/quarterly-addenda-updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71878598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 1711 / 2015 rows.\n",
      "service_config.json UPDATED (1 bundle(s) changed). Backup saved.\n",
      "MUSC Health test processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# --------------- MUSC HEALTH  ---------------- ADDENDUM B NEEDED\n",
    "# ======================================================================\n",
    "\n",
    "import re\n",
    "from scripts.cleaners import standardize_payer_name\n",
    "from scripts.cleaners import apply_payer_standardization_to_json\n",
    "from scripts.bundle_validation import ValidateJSON\n",
    "from scripts.merge_cpt_to_apc import map_prices_to_hcpcs, load_addendum_b\n",
    "\n",
    "# Grab row for MUSC Health in Charleston, SC\n",
    "hos_name = 'MUSC Health'\n",
    "matching_hospitals = hospitals_df[hospitals_df['name'] == hos_name]\n",
    "if not matching_hospitals.empty:\n",
    "    musc_row = matching_hospitals.iloc[0]\n",
    "else:\n",
    "    print(f\"Hospital '{hos_name}' not found in the dataset\")\n",
    "    musc_row = None\n",
    "\n",
    "# grab json path for MUSC Health\n",
    "musc_json_path = musc_row['json_path']\n",
    "\n",
    "# load a single csv file from csv_folder for testing\n",
    "musc_csv_path = os.path.join(csv_folder, 'MUSC_Health_Medical_Center_CM.csv')\n",
    "musc_df = pd.read_csv(musc_csv_path)\n",
    "\n",
    "# Change all mixed type columns to string to avoid dtype issues (focus codes columns only)\n",
    "code_cols = [c for c in musc_df.columns if c.startswith(\"code|\") or c.startswith(\"code_\")]\n",
    "musc_df[code_cols] = musc_df[code_cols].astype(str)\n",
    "\n",
    "# ======================================================\n",
    "# ADDENDUM B LOADING\n",
    "# ======================================================\n",
    "\n",
    "# load addendum b for mapping, save folder as csv files are stored locally outside of CLEAR repo\n",
    "addendum_b_path = os.path.join(csv_folder, '2025_Web_Addendum_B.csv')\n",
    "\n",
    "# load addendum b\n",
    "addendum_b = load_addendum_b(addendum_b_path)\n",
    "\n",
    "# Map prices to hcpcs codes in musc_df\n",
    "musc_df = map_prices_to_hcpcs(musc_df, addendum_b, expand=True)\n",
    "\n",
    "# replace column name instances with | to _, code|1 becomes code_1, etc\n",
    "musc_df.columns = [re.sub(r'\\|', '_', col) for col in musc_df.columns]\n",
    "\n",
    "# Code structure is similar to Duke with there being multiple code columns\n",
    "\"\"\"\n",
    "Checking matches for column: code_1\n",
    "  HCPCS matches: 0\n",
    "  CPT matches: 0\n",
    "  Lab matches: 0\n",
    "Checking matches for column: code_2\n",
    "  HCPCS matches: 0\n",
    "  CPT matches: 10\n",
    "  Lab matches: 0\n",
    "Checking matches for column: code_3\n",
    "  HCPCS matches: 8\n",
    "  CPT matches: 0\n",
    "  Lab matches: 323\n",
    "Checking matches for column: code_4\n",
    "  HCPCS matches: 0\n",
    "  CPT matches: 0\n",
    "  Lab matches: 0\n",
    "\n",
    "Unique values in code_1_type: ['APC' 'CDM' 'MS-DRG' 'NDC']\n",
    "Unique values in code_2_type: ['nan' 'RC']\n",
    "Unique values in code_3_type: ['nan' 'HCPCS']\n",
    "Unique values in code_4_type: ['nan' 'NDC']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# now grab matches\n",
    "hcpcs_matches = musc_df[musc_df['code_3'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = musc_df[musc_df['code_2'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = musc_df[musc_df['code_3'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]  \n",
    "\n",
    "all_matches = pd.concat(match_dfs, ignore_index=True)\n",
    "\n",
    "# drop all rows where payer_name is null/empty, for now dont worry about est. price, drop duplicates after\n",
    "all_matches = all_matches[(all_matches['payer_name'].notna() & (all_matches['payer_name'] != ''))]\n",
    "all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "# due to the mapping code types are a bit trickier now, espeically to keep records unique, for \n",
    "# now lets just use code_3/code_3_type if present, else code_2/code_2_type for code/type columns\n",
    "def select_code(row):\n",
    "    if pd.notna(row['code_3']) and row['code_3'] != '':\n",
    "        return pd.Series({'code': row['code_3'], 'type': row['code_3_type']})\n",
    "    elif pd.notna(row['code_2']) and row['code_2'] != '':\n",
    "        return pd.Series({'code': row['code_2'], 'type': row['code_2_type']})\n",
    "    else:\n",
    "        return pd.Series({'code': None, 'type': None})\n",
    "    \n",
    "if not all_matches.empty:\n",
    "    all_matches[['code', 'type']] = all_matches.apply(select_code, axis=1)\n",
    "    all_matches = all_matches.drop(columns=['code_2', 'code_2_type', 'code_3', 'code_3_type'])\n",
    "\n",
    "# now lets apply payer standardization\n",
    "all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "# Now lets remove enteries that don't have est. prices\n",
    "all_matches = all_matches[all_matches['estimated_amount'].notna() & (all_matches['estimated_amount'] != '')]\n",
    "\n",
    "# finally export to json\n",
    "all_matches.to_json(musc_json_path, orient='records', lines=True)\n",
    "\n",
    "# Validate the final json\n",
    "validator = ValidateJSON(musc_json_path)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del musc_df\n",
    "del musc_row\n",
    "del musc_csv_path\n",
    "del all_matches\n",
    "\n",
    "print(\"MUSC Health test processing complete.\")\n",
    "\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b4b32",
   "metadata": {},
   "source": [
    "***\n",
    "## Virgina\n",
    "\n",
    "This will be the firs time tesing the all in one code book, where we crosswalked with the RVU to get info about CPT codes then updated their description and compiled into a singular doc. Here is the output from testing:\n",
    "\n",
    "#### Original Method:\n",
    "```python\n",
    "# column 1 matches\n",
    "hcpcs_matches = inova_transformed[inova_transformed['code_1'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = inova_transformed[inova_transformed['code_1'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = inova_transformed[inova_transformed['code_1'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "# column 2 matches\n",
    "hcpcs_matches_2 = inova_transformed[inova_transformed['code_2'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches_2 = inova_transformed[inova_transformed['code_2'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches_2 = inova_transformed[inova_transformed['code_2'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "# print out match counts for each\n",
    "print(f\"Column code_1: HCPCS matches: {len(hcpcs_matches)}, CPT matches: {len(cpt_matches)}, Lab matches: {len(lab_matches)}\")\n",
    "print(f\"Column code_2: HCPCS matches: {len(hcpcs_matches_2)}, CPT matches: {len(cpt_matches_2)}, Lab matches: {len(lab_matches_2)}\")\n",
    "```\n",
    "- Column code_1: HCPCS matches: 0, CPT matches: 156, Lab matches: 0\n",
    "- Column code_2: HCPCS matches: 8054, CPT matches: 4275, Lab matches: 327\n",
    "\n",
    "#### New Spreadsheet Method:\n",
    "\n",
    "```python\n",
    "# column 1 matches\n",
    "hcpcs_filtered_codes = all_codes[all_codes['source'] == 'HCPCS']\n",
    "cpt_filtered_codes = all_codes[all_codes['source'] == 'CPT']\n",
    "lab_filtered_codes = all_codes[all_codes['source'] == 'Lab']\n",
    "\n",
    "# column 1 matches\n",
    "hcpcs_matches = inova_transformed[inova_transformed['code_1'].isin(hcpcs_filtered_codes['code'])]\n",
    "cpt_matches = inova_transformed[inova_transformed['code_1'].isin(cpt_filtered_codes['code'])]\n",
    "lab_matches = inova_transformed[inova_transformed['code_1'].isin(lab_filtered_codes['code'])]\n",
    "\n",
    "# column 2 matches\n",
    "hcpcs_matches_2 = inova_transformed[inova_transformed['code_2'].isin(hcpcs_filtered_codes['code'])]\n",
    "cpt_matches_2 = inova_transformed[inova_transformed['code_2'].isin(cpt_filtered_codes['code'])]\n",
    "lab_matches_2 = inova_transformed[inova_transformed['code_2'].isin(lab_filtered_codes['code'])]\n",
    "\n",
    "# print out match counts for each\n",
    "print(f\"Column code_1: HCPCS matches: {len(hcpcs_matches)}, CPT matches: {len(cpt_matches)}, Lab matches: {len(lab_matches)}\")\n",
    "print(f\"Column code_2: HCPCS matches: {len(hcpcs_matches_2)}, CPT matches: {len(cpt_matches_2)}, Lab matches: {len(lab_matches_2)}\")\n",
    "```\n",
    "- Column code_1: HCPCS matches: 0, CPT matches: 0, Lab matches: 0\n",
    "- Column code_2: HCPCS matches: 8054, CPT matches: 2345, Lab matches: 288\n",
    "\n",
    "We can see tha the original is catching more CPT matches as well as Lab, but there is a chance that it's catching codes that are depreciated. This needs to be investigated further but I already have enough on my plate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming Inova Fairfax data from wide to long format...\n",
      "Inova Fairfax Hospital test processing complete.\n",
      "Inova Fairfax Hospital test processing complete.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "#  Inova Fairfax Hospital \n",
    "# =====================================================================\n",
    "\n",
    "from scripts.cleaners import transform_wide_to_long_format\n",
    "\n",
    "hos_name = 'Inova Fairfax Hospital'\n",
    "address = '3300 Gallows Rd'\n",
    "city_name = 'Falls Church'\n",
    "state_name = 'VA'\n",
    "zip_code = '22042'\n",
    "\n",
    "# update hospitals_df with new entry if it doesn't already exist\n",
    "# move this to be a function later\n",
    "if hospitals_df[\n",
    "    (hospitals_df['name'] == hos_name) &\n",
    "    (hospitals_df['state'] == state_name) &\n",
    "    (hospitals_df['city'] == city_name)\n",
    "].empty:\n",
    "    new_entry = {\n",
    "        'name': hos_name,\n",
    "        'address': address,\n",
    "        'city': city_name,\n",
    "        'state': state_name,\n",
    "        'zip': zip_code\n",
    "    }\n",
    "    hospitals_df = pd.concat([hospitals_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
    "    hospitals_df.to_csv(hospitals_csv, index=False)  # Save updated CSV\n",
    "    print(f\"Added new hospital entry for '{hos_name}' to hospitals.csv\")\n",
    "\n",
    "    # now run the update_dataframe function to add lat/lon, id, and json_path\n",
    "    update_dataframe(hospitals_df)\n",
    "\n",
    "# now lets grab all the paths\n",
    "inova_json_path = hospitals_df[hospitals_df['name'] == hos_name]['json_path'].values[0]\n",
    "inova_csv_path = os.path.join(csv_folder, 'INOVA_FAIRFAX_CM.csv')\n",
    "\n",
    "# load Inova Fairfax Hospital csv\n",
    "inova_df = pd.read_csv(inova_csv_path)\n",
    "\n",
    "# Inova is another wide format CM structure, so we need to transform it\n",
    "# Apply the transformation\n",
    "print(\"Transforming Inova Fairfax data from wide to long format...\")\n",
    "inova_transformed = transform_wide_to_long_format(inova_df, )\n",
    "\n",
    "# Original shape: (19293, 248)\n",
    "# Transformed shape: (145308, 16)\n",
    "\n",
    "# Now to grab matches\n",
    "# The bulk of the matches are in code_2/code_2_type, so we will just use those for now due to limited space\n",
    "# HCPCS matches\n",
    "hcpcs_matches = inova_transformed[inova_transformed['code_2'].isin(hcpcs_codes['HCPCS Code'])].copy()\n",
    "hcpcs_matches['code'] = hcpcs_matches['code_2']\n",
    "hcpcs_matches['type'] = hcpcs_matches['code_2_type']\n",
    "hcpcs_matches = hcpcs_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# CPT matches\n",
    "cpt_matches = inova_transformed[inova_transformed['code_1'].isin(cpt_codes['HCPCS Code'])].copy()\n",
    "cpt_matches['code'] = cpt_matches['code_2']\n",
    "cpt_matches['type'] = cpt_matches['code_2_type']\n",
    "cpt_matches = cpt_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# Lab matches\n",
    "lab_matches = inova_transformed[inova_transformed['code_2'].isin(lab_codes['HCPCS Code'])].copy()\n",
    "lab_matches['code'] = lab_matches['code_2']\n",
    "lab_matches['type'] = lab_matches['code_2_type']\n",
    "lab_matches = lab_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "    all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "    # redrop possible duplicates\n",
    "    all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "# now lets drop some more columns to save space\n",
    "cols_to_drop = ['billing_class', 'drug_unit_of_measurement', 'drug_type_of_measurement', \n",
    "                'modifiers', 'standard_charge_algorithm', 'additional_generic_notes', 'methodology']\n",
    "all_matches = all_matches.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Now we can save to json\n",
    "all_matches.to_json(inova_json_path, orient='records', lines=True)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del inova_df\n",
    "del inova_transformed\n",
    "del all_matches\n",
    "del inova_csv_path\n",
    "del inova_json_path\n",
    "\n",
    "print(\"Inova Fairfax Hospital test processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3564a0f",
   "metadata": {},
   "source": [
    "***\n",
    "## Medicare Pricing Integration\n",
    "\n",
    "Testing the new enhanced pricing reader that can automatically parse CMS pricing files and match codes from our top 200 lists to Medicare rates. This includes ASC, ASP, CLFS, DMEPOS, PFALL pricing sources plus calculated anesthesia rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2c05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pricing file readers...\n",
      "Testing individual parsers...\n",
      "ASC test: 4723 records loaded\n",
      "  Sample: 0101T = $240.17\n",
      "ASP test: 871 records loaded\n",
      "  Sample: 90653 = $83.49\n",
      "CLFS test: 1926 records loaded\n",
      "  Sample: 0001U = $720.0\n",
      "DMEPOS test: 2059 records loaded\n",
      "  Sample: A4216 = $0.97\n",
      "PFALL test: 7481 records loaded\n",
      "  Sample: G0011 = $31.56\n",
      "Anesthesia test: 5 base units + 68 minutes = $208.62\n",
      "PFALL test: 7481 records loaded\n",
      "  Sample: G0011 = $31.56\n",
      "Anesthesia test: 5 base units + 68 minutes = $208.62\n"
     ]
    }
   ],
   "source": [
    "# Test the enhanced pricing reader functionality\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Reload the module to pick up changes\n",
    "if 'scripts.enhanced_pricing_reader' in sys.modules:\n",
    "    importlib.reload(sys.modules['scripts.enhanced_pricing_reader'])\n",
    "\n",
    "from scripts.enhanced_pricing_reader import match_codes_to_pricing, test_pricing_reader\n",
    "\n",
    "# First, let's test the individual pricing file readers\n",
    "print(\"Testing pricing file readers...\")\n",
    "pricing_folder_path = os.path.join(workspace_root, '..', 'ChargeMaster_Project', 'pricing_info')\n",
    "pricing_folder_path = os.path.abspath(pricing_folder_path)\n",
    "\n",
    "# Run the test function\n",
    "test_pricing_reader(pricing_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf287e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MATCHING CODES TO MEDICARE PRICING\n",
      "============================================================\n",
      "Starting code matching process...\n",
      "Parsing all pricing files...\n",
      "ASC: 4723 records\n",
      "ASP: 871 records\n",
      "CLFS: 1926 records\n",
      "DMEPOS: 2059 records\n",
      "PFALL: 7481 records\n",
      "Total unique codes to match: 478\n",
      "PFALL: 7481 records\n",
      "Total unique codes to match: 478\n",
      "\n",
      "Matching Results:\n",
      "Total codes processed: 478\n",
      "Successfully matched: 450\n",
      "Unmatched codes: 28\n",
      "Match rate: 94.1%\n",
      "\n",
      "Pricing sources used:\n",
      "  PFALL: 189 codes\n",
      "  ASC: 96 codes\n",
      "  CLFS: 79 codes\n",
      "  ASP: 48 codes\n",
      "  DMEPOS: 38 codes\n",
      "\n",
      "Results saved to: c:\\Users\\jcing\\OneDrive\\Desktop\\MADS\\DATA 760\\Project\\CLEAR\\docs\\data\\medicare_pricing_matched.csv\n",
      "Unmatched codes saved to: c:\\Users\\jcing\\OneDrive\\Desktop\\MADS\\DATA 760\\Project\\CLEAR\\docs\\data\\medicare_pricing_matched_unmatched.csv\n",
      "\n",
      "Matched pricing data shape: (450, 4)\n",
      "Price range: $0.01 - $14070.52\n",
      "Average price: $457.20\n",
      "\n",
      "Sample matched results:\n",
      " code   price source\n",
      "87801  70.200   CLFS\n",
      "Q5101   0.360    ASC\n",
      "J0461   0.107    ASP\n",
      "J7209   1.240    ASC\n",
      "A6216   0.090 DMEPOS\n",
      "A5063   4.470 DMEPOS\n",
      "87186   8.650   CLFS\n",
      "J1437  20.770    ASC\n",
      "88374 364.100  PFALL\n",
      "J9264  13.540    ASC\n",
      "\n",
      "Matching Results:\n",
      "Total codes processed: 478\n",
      "Successfully matched: 450\n",
      "Unmatched codes: 28\n",
      "Match rate: 94.1%\n",
      "\n",
      "Pricing sources used:\n",
      "  PFALL: 189 codes\n",
      "  ASC: 96 codes\n",
      "  CLFS: 79 codes\n",
      "  ASP: 48 codes\n",
      "  DMEPOS: 38 codes\n",
      "\n",
      "Results saved to: c:\\Users\\jcing\\OneDrive\\Desktop\\MADS\\DATA 760\\Project\\CLEAR\\docs\\data\\medicare_pricing_matched.csv\n",
      "Unmatched codes saved to: c:\\Users\\jcing\\OneDrive\\Desktop\\MADS\\DATA 760\\Project\\CLEAR\\docs\\data\\medicare_pricing_matched_unmatched.csv\n",
      "\n",
      "Matched pricing data shape: (450, 4)\n",
      "Price range: $0.01 - $14070.52\n",
      "Average price: $457.20\n",
      "\n",
      "Sample matched results:\n",
      " code   price source\n",
      "87801  70.200   CLFS\n",
      "Q5101   0.360    ASC\n",
      "J0461   0.107    ASP\n",
      "J7209   1.240    ASC\n",
      "A6216   0.090 DMEPOS\n",
      "A5063   4.470 DMEPOS\n",
      "87186   8.650   CLFS\n",
      "J1437  20.770    ASC\n",
      "88374 364.100  PFALL\n",
      "J9264  13.540    ASC\n"
     ]
    }
   ],
   "source": [
    "# Now let's match our loaded codes to Medicare pricing\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MATCHING CODES TO MEDICARE PRICING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare the list of code dataframes\n",
    "code_dataframes = [hcpcs_codes, lab_codes, cpt_codes, all_codes]\n",
    "\n",
    "# Match codes to pricing and create unified output\n",
    "output_path = os.path.join(workspace_root, 'docs', 'data', 'medicare_pricing_matched.csv')\n",
    "matched_pricing = match_codes_to_pricing(\n",
    "    code_dataframes=code_dataframes,\n",
    "    pricing_folder=pricing_folder_path,\n",
    "    output_file=output_path,\n",
    "    include_anesthesia=True\n",
    ")\n",
    "\n",
    "print(f\"\\nMatched pricing data shape: {matched_pricing.shape}\")\n",
    "if not matched_pricing.empty:\n",
    "    print(f\"Price range: ${matched_pricing['price'].min():.2f} - ${matched_pricing['price'].max():.2f}\")\n",
    "    print(f\"Average price: ${matched_pricing['price'].mean():.2f}\")\n",
    "    \n",
    "    # Show sample of results\n",
    "    print(f\"\\nSample matched results:\")\n",
    "    print(matched_pricing.head(10)[['code', 'price', 'source']].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
