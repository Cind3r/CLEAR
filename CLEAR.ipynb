{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a97bcb8",
   "metadata": {},
   "source": [
    "# CLEAR\n",
    "\n",
    "Chargemaster Location-based Exploration for Affordability & Reform\n",
    "\n",
    "This is the notebook file primarily responsible for pre-processing data, attaching important information, and generating database files for the github page. Below you can find all information about how data is processed from the downloaded `.csv` files found on most hospital sites. This is an exploratory project focused on creating interactive visualzations and tools to better inform people about their healthcare. The repo can always be maintained by downloading the most current year data for the specific hospital and putting it through the scripts. It should be noted that this is NOT a comprehensive list, but it can potentially be scaled to a full working-standalone site with enough time. \n",
    "\n",
    "All pre-processing code is written in python. See the `.html` files for how the D3 visualizations work. \n",
    "\n",
    "## How it works (Copied from README)\n",
    "\n",
    "Hospitals that have been added to this 'web-app' are stored in a `.csv` file for quick look up and ease of access. This points to the loc of it's Charge Master `.json` file which is then queried for the specific procedure. Hospitals are gathered from the CSV list based on a radius look-up provided by the user. If a hospital in the radius does not offer the service, it will not display the price point compared to others in the radius. \n",
    "\n",
    "Currently limited to 500 procedures due to file size limits and me not wanting to set up a server/database for this. Parquet only works server side so i can't do iterative testing before publishing to pages, and pages deployments can take a while. Will consider moving to parquet system after front-end is stable and working as envisioned.\n",
    "\n",
    "## List of Hospitals\n",
    "\n",
    "These are the hospital's which data has been gathered and processed for thus far:\n",
    "\n",
    "| State    | Hospital Name                     | Zipcode     | Date                 | File Size    | Link                                                            |\n",
    "|----------|--------------------------------|-------------|-------------------|-------------|------------------------------------------------|\n",
    "| NC        | Duke University Hospital     |     27710    |      09/2025      |   3.32 GB   |    [Link](https://www.dukehealth.org/paying-for-care/what-duke-charges-services) |\n",
    "| NC        | AdventHealth (Hendersonville)   |     28792   |   09/2025    |      1.48 GB           |                                                                 |\n",
    "| NC | UNC Rex Hospital | 27606 | 09/2025 | 121 MB | [Link](https://www.unchealth.org/records-insurance/standard-charges) |\n",
    "| SC        | MUSC Health-University Medical Center (Charleston) |   29425   | 09/2025 | 11.8 MB |  [Link](https://muschealth.org/patients-visitors/billing/price-transparency) |\n",
    "\n",
    "#### Top Hospitals in Every State\n",
    "\n",
    "- Alaska: Providence Alaska Medical Center (Anchorage) and Fairbanks Memorial Hospital\n",
    "- Alabama: University of Alabama at Birmingham Hospital        \n",
    "- Arizona: Mayo Clinic-Phoenix        \n",
    "- Arkansas: Washington Regional Medical Center (Fayetteville)        \n",
    "- California: Cedars-Sinai Medical Center (Los Angeles), UCLA Medical Center (Los Angeles), Stanford Health Care-Stanford Hospital (Palo Alto), UC San Diego Health-LaJolla and Hillcrest Hospitals, and UCSF Health-UCSF Medical Center (San Francisco)\n",
    "- Colorado: UCHealth University of Colorado Hospital (Aurora)        \n",
    "- Connecticut: Yale New Haven Hospital        \n",
    "- Delaware: ChristianaCare Hospitals (Newark)        \n",
    "- Florida: Mayo Clinic-Jacksonville        \n",
    "- Georgia: Emory University Hospital (Atlanta)        \n",
    "- Hawaii: Queen’s Medical Center (Honolulu)        \n",
    "- Idaho: St. Luke’s Regional Medical Center (Boise)        \n",
    "- Illinois: Northwestern Medicine-Northwestern Memorial Hospital (Chicago) and Rush University Hospital (Chicago)         \n",
    "- Indiana: Indiana University Health Medical Center (Indianapolis)        \n",
    "- Iowa: University of Iowa Hospitals and Clinics (Iowa City)        \n",
    "- Kansas: University of Kansas Hospital (Kansas City)        \n",
    "- Kentucky: University of Kentucky Albert B. Chandler Hospital (Lexington) \n",
    "- Louisiana: Ochsner Medical Center (New Orleans)        \n",
    "- Maine: Maine Medical Center (Portland)        \n",
    "- Maryland: Johns Hopkins Hospital (Baltimore)        \n",
    "- Massachusetts: Massachusetts General Hospital (Boston) and Brigham and Women’s Hospital (Boston)       \n",
    "- Michigan: University of Michigan Health-Ann Arbor        \n",
    "- Minnesota: Mayo Clinic (Rochester)        \n",
    "- Mississippi: Mississippi Baptist Medical Center (Jackson)        \n",
    "- Missouri: Barnes-Jewish Hospital (St. Louis)        \n",
    "- Montana: Billings Clinic        \n",
    "- Nebraska: Nebraska Medicine-Nebraska Medical Center (Omaha)        \n",
    "- Nevada: Renown Regional Medical Center (Reno)        \n",
    "- New Hampshire: Dartmouth Hitchcock Medical Center (Lebanon)        \n",
    "- New Jersey: Hackensack University Medical Center at Hackensack University Health      \n",
    "- New Mexico: Presbyterian Hospital (Albuquerque)        \n",
    "- New York: NYU Langone Hospitals (New York City), New York-Presbyterian Hospital-Columbia and Cornell (New York City), Mount Sinai Hospital (New York City), and North - Shore University Hospital at Northwell Health (Manhasset)        \n",
    "- **North Carolina: Duke University Hospital (Durham)**       \n",
    "- North Dakota: Sanford Medical Center Fargo        \n",
    "- Ohio: Cleveland Clinic        \n",
    "- Oklahoma: St. Francis Hospital-Tulsa        \n",
    "- Oregon: OHSU Hospital (Portland)        \n",
    "- Pennsylvania: Hospitals of the University of Pennsylvania-Penn Presbyterian (Philadelphia)        \n",
    "- Rhode Island: Miriam Hospital (Providence)        \n",
    "- South Carolina: MUSC Health-University Medical Center (Charleston)        \n",
    "- South Dakota: Sanford USD Medical Center (Sioux Falls)        \n",
    "- Tennessee: Vanderbilt University Medical Center (Nashville)        \n",
    "- Texas: Houston Methodist Hospital and UT Southwestern Medical Center (Dallas)\n",
    "- Utah: University of Utah Hospital (Salt Lake City)        \n",
    "- Vermont: University of Vermont Medical Center (Burlington)        \n",
    "- Virginia: Inova Fairfax Hospital (Falls Church)        \n",
    "- Washington: UW Medicine-University of Washington Medical Center (Seattle)        \n",
    "- West Virginia: West Virginia University Hospitals (Morgantown)        \n",
    "- Wisconsin: UW Health University Hospital (Madison)\n",
    "\n",
    "As by Becker https://www.beckershospitalreview.com/rankings-and-ratings/us-news-top-hospitals-by-state-for-2023-24/\n",
    "\n",
    "## Outside Sources Used\n",
    "\n",
    "- zip_centroids.csv courtesy of SimpleMaps data https://simplemaps.com/data/us-zips.\n",
    "- CMS.gov data for top 200 HCPCS and CPT codes billed for 2024 & top 100 lab codes. [Link](https://www.cms.gov/data-research/statistics-trends-and-reports/medicare-fee-for-service-parts-a-b/medicare-utilization-part-b)\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bdabc1",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "CSV files are too large to store on github, thus they are downloaded locally, converted to the necessary format, then uploaded. If you want to perform conversions yourself you will need to find the specific hospital chargemaster and document in the notebook accordingly.\n",
    "\n",
    "Not all Charge Masters (CM) are formatted the same, as such, to keep this notebook from growing too large, custom python scripts will be made for unique CM's. This matters beccause some hospitals are regional or statewide 'chains' but can vary prices between locations. For example, \n",
    "\n",
    "**AdventHealth**\n",
    "- AdventHealth Orlando\n",
    "- AdventHealth Tampa\n",
    "- AdventHealth Hendersonville\n",
    "\n",
    "all are AdventHealth hospitals, but their prices and available procedures vary per location. However, the same script to clean and process their CM's works because the file structure doesn't change from loc to loc. Normally CM structure only changes from hospital to hospital (brand-wise), but I haven't looked at the majority of US hospitals so this statement might need to be amended. \n",
    "\n",
    "Think of this file as more of a \"**Controller**\" for the cleaning, while the cleaning process is performed by imported functions. Subsections from here on are labeled by State, be sure to check which Hospitals are in each subsection before uploading data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bf0c7",
   "metadata": {},
   "source": [
    "***\n",
    "## Payer & Plan Names\n",
    "\n",
    "Naming conventions for payer/plans differ across hospitals, making this a pain. Like is an exhaustive regex section to hopefully simplify this so that the functionality of the .html page remains. \n",
    "\n",
    "Idk where this fits in, I'll add it later to documentation.\n",
    "\n",
    "use this to create a comprehensive list of all current payer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2519a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AdventHealth_Hendersonville_CM.csv. Dropping dataframe from memory...\n",
      "Processed AdventHealth_Orlando_CM.csv. Dropping dataframe from memory...\n",
      "Processed AdventHealth_Tampa_CM.csv. Dropping dataframe from memory...\n",
      "Processed DukeHospital_Durham.csv. Dropping dataframe from memory...\n",
      "Unique payer and plan names extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "# imports necessary for data regex searching and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# csv's are stored locally outside of CLEAR repo\n",
    "# set up one folder then into 'ChargeMaster_Project/csv_files/'\n",
    "# get path to csv_files folder outside CLEAR repo\n",
    "workspace_root = os.path.dirname(os.path.abspath('CLEAR.ipynb'))\n",
    "csv_folder = os.path.join(workspace_root, '..', 'ChargeMaster_Project', 'csv_files')\n",
    "csv_folder = os.path.abspath(csv_folder)\n",
    "\n",
    "# lets load all CM files into pandas dataframes, grab the unique values for 'payer_name' and 'plan_name' columns only\n",
    "# then drop everything else to save space\n",
    "for file in os.listdir(csv_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_folder, file)\n",
    "        df = pd.read_csv(file_path, dtype=str)  # read all columns as strings to avoid dtype issues\n",
    "        if 'payer_name' in df.columns and 'plan_name' in df.columns:\n",
    "            unique_payers = df['payer_name'].dropna().unique()\n",
    "            unique_plans = df['plan_name'].dropna().unique()\n",
    "            # save unique payers and plans to text files for later use\n",
    "            with open(os.path.join(csv_folder, f'{file}_unique_payers.txt'), 'w') as f:\n",
    "                for payer in unique_payers:\n",
    "                    f.write(f\"{payer}\\n\")\n",
    "            with open(os.path.join(csv_folder, f'{file}_unique_plans.txt'), 'w') as f:\n",
    "                for plan in unique_plans:\n",
    "                    f.write(f\"{plan}\\n\")\n",
    "        del df  # drop dataframe to save memory\n",
    "        print(f\"Processed {file}. Dropping dataframe from memory...\")\n",
    "print(\"Unique payer and plan names extracted and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ce11e",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Preloading Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospitals.csv updater/editor\n",
    "import hashlib\n",
    "import requests\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import nbformat\n",
    "from scripts.cleaners import apply_payer_standardization_to_json, standardize_payer_name\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"CLEAR-geoapi-2025\")\n",
    "csv_file = 'docs/data/hospitals.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# construct address for geocoding only (don't modify original data)\n",
    "def construct_geocoding_address(row):\n",
    "    # Build clean address from original components\n",
    "    address = f\"{row['address']}, {row['city']}, {row['state']} {row['zip']}\"\n",
    "    return address\n",
    "\n",
    "# get lat/lon from address with increased timeout and retry/delay\n",
    "def get_lat_lon(address, max_retries=3, delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            location = geolocator.geocode(address, timeout=5)\n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "            else:\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding {address} (attempt {attempt+1}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None, None\n",
    "\n",
    "# generate short unique ID based on ['hospital'] + full composite address (base36, 8 chars)\n",
    "def generate_short_id(row):\n",
    "    full_address = construct_geocoding_address(row)\n",
    "    unique_string = f\"{row['name']}_{full_address}\"\n",
    "    hash_int = int(hashlib.md5(unique_string.encode()).hexdigest(), 16)\n",
    "    short_id = base36encode(hash_int)[:8]\n",
    "    return short_id\n",
    "\n",
    "# base36 encoding for shorter IDs\n",
    "def base36encode(number):\n",
    "    chars = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    if number == 0:\n",
    "        return '0'\n",
    "    result = ''\n",
    "    while number > 0:\n",
    "        number, i = divmod(number, 36)\n",
    "        result = chars[i] + result\n",
    "    return result\n",
    "\n",
    "# Add lat/lon and short_id to dataframe, set json_path to be '/data/prices/['state']/['id'].json'\n",
    "def update_dataframe(df):\n",
    "    \n",
    "    # Don't modify the address column - just use it for geocoding\n",
    "    def lat_lon_with_delay(row):\n",
    "        geocoding_address = construct_geocoding_address(row)\n",
    "        lat, lon = get_lat_lon(geocoding_address)\n",
    "        time.sleep(1)  # 1 second delay per request\n",
    "        return pd.Series([lat, lon])\n",
    "    \n",
    "    df[['lat', 'lon']] = df.apply(lat_lon_with_delay, axis=1)\n",
    "    df['id'] = df.apply(generate_short_id, axis=1)\n",
    "    df['json_path'] = df.apply(lambda row: f\"docs/data/prices/{row['state']}/{row['id']}.json\", axis=1)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "update_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5d6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to create comparison df's for the top 200 HCPCS and CMS codes billed for 2024 & top 100 lab codes\n",
    "# first load the codes from the .csv files\n",
    "hcpcs_codes = pd.read_csv('docs/data/hcpcs_lvl2_top_200_codes_2024.csv')\n",
    "lab_codes = pd.read_csv('docs/data/lab_top_100_codes_2024.csv')\n",
    "cpt_codes = pd.read_csv('docs/data/cpt_lvl1_top_200_codes_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48eb048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TO LOAD HOSPITALS CSV\n",
    "import os\n",
    "\n",
    "# csv's are stored locally outside of CLEAR repo\n",
    "# set up one folder then into 'ChargeMaster_Project/csv_files/'\n",
    "# get path to csv_files folder outside CLEAR repo\n",
    "workspace_root = os.path.dirname(os.path.abspath('CLEAR.ipynb'))\n",
    "csv_folder = os.path.join(workspace_root, '..', 'ChargeMaster_Project', 'csv_files')\n",
    "csv_folder = os.path.abspath(csv_folder)\n",
    "\n",
    "# define path to hospitals.csv\n",
    "hospitals_csv = os.path.join(workspace_root, 'docs', 'data', 'hospitals.csv')\n",
    "hospitals_csv = os.path.abspath(hospitals_csv)\n",
    "\n",
    "# read hospitals.csv to get list of hospitals and their file paths\n",
    "hospitals_df = pd.read_csv(hospitals_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finish implementation later and move\n",
    "def update_hospital_table():\n",
    "    # Update the first markdown cell's hospital table with new hospitals from hospitals.csv\n",
    "    notebook_path = 'CLEAR.ipynb'\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "    # Find the first markdown cell with the hospital table\n",
    "    table_md_header = \"| State    | Hospital Name\"\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown' and table_md_header in cell.source:\n",
    "            lines = cell.source.splitlines()\n",
    "            # Find start and end of the table\n",
    "            table_start = next(i for i, l in enumerate(lines) if l.strip().startswith(table_md_header))\n",
    "            table_rows = lines[table_start+2:]  # skip header and separator\n",
    "            existing_names = set()\n",
    "            for row in table_rows:\n",
    "                parts = row.split('|')\n",
    "                if len(parts) > 2:\n",
    "                    existing_names.add(parts[2].strip())\n",
    "            # Add new hospitals not already in the table\n",
    "            new_rows = []\n",
    "            for _, row in hospitals_df.iterrows():\n",
    "                if row['name'] not in existing_names:\n",
    "                    new_rows.append(f\"| {row['state']} | {row['name']} | {row['zip']} |  |  |\")\n",
    "            # Insert new rows after the last table row\n",
    "            updated_lines = lines[:table_start+2] + table_rows + new_rows\n",
    "            cell.source = \"\\n\".join(updated_lines)\n",
    "            break\n",
    "\n",
    "    # Save the updated notebook\n",
    "    with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208356c9",
   "metadata": {},
   "source": [
    "***\n",
    "## North Carolina Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ff6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcing\\AppData\\Local\\Temp\\ipykernel_5708\\1631332753.py:19: DtypeWarning: Columns (7,8,9,12,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  duke_df = pd.read_csv(test_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duke Hospital test processing complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================================\n",
    "# --------------- DUKE HOSPITAL TESTING ----------------\n",
    "# ======================================================================\n",
    "\n",
    "# Grab row for Duke Hospital in Durham, NC\n",
    "hos_name = 'Duke University Hospital'\n",
    "matching_hospitals = hospitals_df[hospitals_df['name'] == hos_name]\n",
    "if not matching_hospitals.empty:\n",
    "    duke_row = matching_hospitals.iloc[0]\n",
    "else:\n",
    "    print(f\"Hospital '{hos_name}' not found in the dataset\")\n",
    "    duke_row = None\n",
    "\n",
    "# grab json path for Duke Hospital\n",
    "duke_json_path = duke_row['json_path']\n",
    "\n",
    "# load a single csv file from csv_folder for testing\n",
    "test_csv_path = os.path.join(csv_folder, 'DukeHospital_Durham.csv')\n",
    "duke_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Change all mixed type columns to string to avoid dtype issues\n",
    "for col in duke_df.columns:\n",
    "    if duke_df[col].dtype == 'object':\n",
    "        duke_df[col] = duke_df[col].astype(str)\n",
    "\n",
    "\n",
    "# remove duke_df Hospital, City, State, Address columns before converting to parquet\n",
    "duke_df = duke_df.drop(columns=['Hospital', 'City', 'State', 'Address'])\n",
    "\n",
    "\n",
    "#code_cols = ['code_1', 'code_2', 'code_3', 'code_4']\n",
    "# Check matches for each code column against hcpcs_codes, cpt_codes, and lab_codes, iteratively\n",
    "# for col in code_cols:\n",
    "#     print(f\"Checking matches for column: {col}\")\n",
    "#     hcpcs_matches = duke_df[duke_df[col].isin(hcpcs_codes['HCPCS Code'])]\n",
    "#     cpt_matches = duke_df[duke_df[col].isin(cpt_codes['HCPCS Code'])]\n",
    "#     lab_matches = duke_df[duke_df[col].isin(lab_codes['HCPCS Code'])]\n",
    "#     print(f\"  HCPCS matches: {len(hcpcs_matches)}\")\n",
    "#     print(f\"  CPT matches: {len(cpt_matches)}\")\n",
    "#     print(f\"  Lab matches: {len(lab_matches)}\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    This actually shows that code_2 contains HCPCS codes and code_3 contains CPT codes\n",
    "    Checking matches for column: code_1\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 0\n",
    "        Lab matches: 0\n",
    "    Checking matches for column: code_2\n",
    "        HCPCS matches: 76966\n",
    "        CPT matches: 0\n",
    "        Lab matches: 19987\n",
    "    Checking matches for column: code_3\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 772\n",
    "        Lab matches: 0\n",
    "    Checking matches for column: code_4\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 0\n",
    "        Lab matches: 0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Duke Hospital CM Structure\n",
    "# code_2/code_3 [columns 3, 5 --> 4, 6 contain type] contain HCPCS and CPT codes, so we use those for comparison against the top 200 lists\n",
    "# Columns 13-24 contain payer, plan, and pricing info, so we want all of those as well as column 0 which is the \n",
    "# description of the code [used for regex matching on the front end]\n",
    "# final columns to keep: 0, 3-6, 13-24\n",
    "duke_df = duke_df.iloc[:, [0] + list(range(3, 7)) + list(range(13, 25))]\n",
    "\n",
    "# actually lets go ahead and drop some columns to conserve space\n",
    "duke_df = duke_df.drop(columns=['standard_charge_algorithm', 'additional_generic_notes'])\n",
    "\n",
    "# now we can search duke_df['code_2'] and duke_df['code_2_type'] against hcpcs_codes , cpt_codes, and lab_codes\n",
    "# first search hcpcs_codes\n",
    "hcpcs_matches = duke_df[duke_df['code_2'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = duke_df[duke_df['code_3'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = duke_df[duke_df['code_2'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "\n",
    "# Apply standardization to all_matches if not empty\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "    all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "    # redrop possible duplicates\n",
    "    all_matches = all_matches.drop_duplicates()\n",
    "else:\n",
    "    # Create empty DataFrame with same structure as duke_df if no matches\n",
    "    all_matches = pd.DataFrame(columns=duke_df.columns)\n",
    "\n",
    "# There are some duplicate issues, mainly rows where no est. price are given, so lets remove enteries that don't have est. prices\n",
    "all_matches = all_matches[all_matches['estimated_amount'].notna() & (all_matches['estimated_amount'] != '')]\n",
    "\n",
    "# Now we need to combine the columns, code_2 is generally more important so we save that over 3 if they're both present\n",
    "# Combine code_2/code_2_type and code_3/code_3_type into 'code' and 'type'\n",
    "def select_code(row):\n",
    "    if pd.notna(row['code_2']) and row['code_2'] != '':\n",
    "        return pd.Series({'code': row['code_2'], 'type': row['code_2_type']})\n",
    "    elif pd.notna(row['code_3']) and row['code_3'] != '':\n",
    "        return pd.Series({'code': row['code_3'], 'type': row['code_3_type']})\n",
    "    else:\n",
    "        return pd.Series({'code': None, 'type': None})\n",
    "\n",
    "all_matches[['code', 'type']] = all_matches.apply(select_code, axis=1)\n",
    "all_matches = all_matches.drop(columns=['code_2', 'code_2_type', 'code_3', 'code_3_type'])\n",
    "all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "# Save output data to json file for Duke json path\n",
    "all_matches.to_json(duke_json_path, orient='records', lines=True)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del duke_df\n",
    "del duke_row\n",
    "del test_csv_path\n",
    "del all_matches\n",
    "\n",
    "print(\"Duke Hospital test processing complete.\")\n",
    "# ======================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e4f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcing\\AppData\\Local\\Temp\\ipykernel_5708\\1743173882.py:27: DtypeWarning: Columns (1,2,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  adv_nc_df = pd.read_csv(adv_nc_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdventHealth Hendersonville, NC test processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# --------------- ADVENTHEALTH HOSPITAL  ----------------\n",
    "# ======================================================================\n",
    "\n",
    "# Load AdventHealth Hendersonville, NC paths\n",
    "hos_name = 'AdventHealth'\n",
    "city_name = 'Hendersonville'\n",
    "state_name = 'NC'\n",
    "matching_hospitals = hospitals_df[\n",
    "    (hospitals_df['name'] == hos_name) &\n",
    "    (hospitals_df['state'] == state_name) &\n",
    "    (hospitals_df['city'] == city_name)\n",
    "]\n",
    "if not matching_hospitals.empty:\n",
    "    adv_nc = matching_hospitals.iloc[0]\n",
    "else:\n",
    "    print(f\"Hospital '{hos_name}' not found in the dataset\")\n",
    "    adv_nc = None\n",
    "\n",
    "# grab json path for AdventHealth Hendersonville, NC\n",
    "adv_nc_json_path = adv_nc['json_path']\n",
    "\n",
    "# load a single csv file from csv_folder for testing\n",
    "adv_nc_csv_path = os.path.join(csv_folder, 'AdventHealth_Hendersonville_CM.csv')\n",
    "\n",
    "# load AdventHealth Hendersonville, NC csv\n",
    "adv_nc_df = pd.read_csv(adv_nc_csv_path)\n",
    "\n",
    "# AdventHealth CM Structure\n",
    "# ['description', 'drug_information', 'code', 'type',\n",
    "#    'standard_charge_min', 'standard_charge_max', 'gross_charge',\n",
    "#    'discounted_cash', 'setting', 'payer_name', 'plan_name',\n",
    "#    'standard_charge_dollar', 'standard_charge_percentage',\n",
    "#    'estimated_amount', 'methodology', 'standard_charge_algorithm',\n",
    "#    'Hospital', 'City', 'State', 'Address']\n",
    "\n",
    "# Check matches for code column against hcpcs_codes, cpt_codes, and lab_codes\n",
    "# Output: \n",
    "# HCPCS matches: 20845\n",
    "#   CPT matches: 2884\n",
    "#   Lab matches: 13555\n",
    "\n",
    "# Lets drop unneeded columns, and rename some before grabing the matches and saving to json\n",
    "# NOTE: common naming convention needs to be added before renaming cols\n",
    "cols_to_drop = ['methodology', 'drug_information', 'standard_charge_algorithm', 'Hospital', 'City', 'State', 'Address']\n",
    "adv_nc_df = adv_nc_df.drop(columns=cols_to_drop)\n",
    "\n",
    "# now grab matches\n",
    "hcpcs_matches = adv_nc_df[adv_nc_df['code'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = adv_nc_df[adv_nc_df['code'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = adv_nc_df[adv_nc_df['code'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "    all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "    # redrop possible duplicates\n",
    "    all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "else:\n",
    "    # Create empty DataFrame with same structure as duke_df if no matches\n",
    "    all_matches = pd.DataFrame(columns=duke_df.columns)\n",
    "\n",
    "# There are some duplicate issues, mainly rows where no est. price are given, so lets remove enteries that don't have est. prices\n",
    "all_matches = all_matches[all_matches['estimated_amount'].notna() & (all_matches['estimated_amount'] != '')]\n",
    "\n",
    "# Save output data to json file for AdventHealth Hendersonville, NC json path\n",
    "all_matches.to_json(adv_nc_json_path, orient='records', lines=True)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del adv_nc_df\n",
    "del adv_nc\n",
    "del adv_nc_csv_path\n",
    "del all_matches\n",
    "\n",
    "print(\"AdventHealth Hendersonville, NC test processing complete.\")\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b0c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcing\\AppData\\Local\\Temp\\ipykernel_19036\\2492547596.py:47: DtypeWarning: Columns (5,6,10,11,19,22,43,46,49,52,55,58,61,64,73,76,88,91,94,109,112,121,124,127,130,133,136,139,142,145,148,151,154,157,160,163,166,169,172,175,178,181,184,187,190,199,202) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  unc_rex_df = pd.read_csv(unc_rex_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming UNC Rex data from wide to long format...\n",
      "Original shape: (160860, 215)\n",
      "Transformed shape: (743258, 23)\n",
      "Matched 6424 / 6568 rows.\n",
      "service_config.json UPDATED (1 bundle(s) changed). Backup saved.\n",
      "UNC Rex Hospital test processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# ---------------  UNC REX HOSPITAL  ----------------\n",
    "# ======================================================================\n",
    "# Lets start adding info to the CSV in python rather than manual edits each time\n",
    "# Load UNC Rex Hospital in Raleigh, NC paths\n",
    "from scripts.cleaners import transform_wide_to_long_format\n",
    "from scripts.bundle_validation import ValidateJSON\n",
    "\n",
    "hos_name = 'UNC Rex Hospital'\n",
    "\n",
    "# add hospital info for UNC Rex to hospitals.csv\n",
    "address = '4420 Lake Boone Trail'\n",
    "city_name = 'Raleigh'\n",
    "state_name = 'NC'\n",
    "zip_code = '27607'\n",
    "\n",
    "# update hospitals_df with new entry if it doesn't already exist\n",
    "# move this to be a function later\n",
    "if hospitals_df[\n",
    "    (hospitals_df['name'] == hos_name) &\n",
    "    (hospitals_df['state'] == state_name) &\n",
    "    (hospitals_df['city'] == city_name)\n",
    "].empty:\n",
    "    new_entry = {\n",
    "        'name': hos_name,\n",
    "        'address': address,\n",
    "        'city': city_name,\n",
    "        'state': state_name,\n",
    "        'zip': zip_code\n",
    "    }\n",
    "    hospitals_df = pd.concat([hospitals_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
    "    hospitals_df.to_csv(hospitals_csv, index=False)  # Save updated CSV\n",
    "    print(f\"Added new hospital entry for '{hos_name}' to hospitals.csv\")\n",
    "\n",
    "    # now run the update_dataframe function to add lat/lon, id, and json_path\n",
    "    update_dataframe(hospitals_df)\n",
    "\n",
    "# now lets grab all the paths\n",
    "unc_rex_json_path = hospitals_df[\n",
    "    (hospitals_df['name'] == hos_name) & \n",
    "    (hospitals_df['state'] == state_name) & \n",
    "    (hospitals_df['city'] == city_name)\n",
    "].iloc[0]['json_path']\n",
    "unc_rex_csv_path = os.path.join(csv_folder, 'UNCREX_CM.csv')\n",
    "\n",
    "# load UNC Rex Hospital csv\n",
    "unc_rex_df = pd.read_csv(unc_rex_csv_path)\n",
    "\n",
    "# UNC Rex CM Structure: COLUMNS -->\n",
    "# description, code|1, code|1|type, code|2, code|2|type, code|3, code|3|type, billing_class, setting,\n",
    "# drug_unit_of_measurement, drug_type_of_measurement, modifiers, standard_charge|gross,\n",
    "# standard_charge|discounted_cash, standard_charge|min, standard_charge|max, additional_generic_notes,\n",
    "# standard_charge|AETNA|CHOICE POS|negotiated_dollar, standard_charge|AETNA|CHOICE\n",
    "# POS|negotiated_percentage, standard_charge|AETNA|CHOICE POS|negotiated_algorithm,\n",
    "# standard_charge|AETNA|CHOICE POS|methodology, estimated_amount|AETNA|CHOICE POS,\n",
    "# additional_payer_notes|AETNA|CHOICE POS, ... etc for other payers/plans\n",
    "\n",
    "# here you can see we have a new type of CM structure where rather than tons of row enteries for each code/payer/plan\n",
    "# we have one row per code with multiple columns for each payer/plan combination, so we need to alter our approach to\n",
    "# processing the data\n",
    "\n",
    "# Apply the transformation\n",
    "print(\"Transforming UNC Rex data from wide to long format...\")\n",
    "unc_rex_transformed = transform_wide_to_long_format(unc_rex_df)\n",
    "\n",
    "print(f\"Original shape: {unc_rex_df.shape}\")\n",
    "print(f\"Transformed shape: {unc_rex_transformed.shape}\")\n",
    "\n",
    "# Great now we can start searching the codes against our top 200 lists\n",
    "# now grab matches\n",
    "\n",
    "\"\"\"\n",
    "Column code_1: HCPCS matches: 0, CPT matches: 128, Lab matches: 0\n",
    "Column code_2: HCPCS matches: 2746, CPT matches: 0, Lab matches: 3719\n",
    "Column code_3: HCPCS matches: 0, CPT matches: 0, Lab matches: 0\n",
    "\"\"\"\n",
    "unc_rex_transformed.drop(['code_3', 'code_3_type'], axis=1, inplace=True)  # drop unused code_3 columns\n",
    "\n",
    "# lets rename the code being grabbed to be code, and type accordingly during the match process\n",
    "# HCPCS matches: filter on code_2, set code='code_2', type='code_2_type'\n",
    "hcpcs_matches = unc_rex_transformed[unc_rex_transformed['code_2'].isin(hcpcs_codes['HCPCS Code'])].copy()\n",
    "hcpcs_matches['code'] = hcpcs_matches['code_2']\n",
    "hcpcs_matches['type'] = hcpcs_matches['code_2_type']\n",
    "hcpcs_matches = hcpcs_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# CPT matches: filter on code_1, set code='code_1', type='code_1_type'\n",
    "cpt_matches = unc_rex_transformed[unc_rex_transformed['code_1'].isin(cpt_codes['HCPCS Code'])].copy()\n",
    "cpt_matches['code'] = cpt_matches['code_1']\n",
    "cpt_matches['type'] = cpt_matches['code_1_type']\n",
    "cpt_matches = cpt_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# Lab matches: filter on code_2, set code='code_2', type='code_2_type'\n",
    "lab_matches = unc_rex_transformed[unc_rex_transformed['code_2'].isin(lab_codes['HCPCS Code'])].copy()\n",
    "lab_matches['code'] = lab_matches['code_2']\n",
    "lab_matches['type'] = lab_matches['code_2_type']\n",
    "lab_matches = lab_matches.drop(columns=['code_1', 'code_1_type', 'code_2', 'code_2_type'])\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "    all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "    # redrop possible duplicates\n",
    "    all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "# now lets drop some more columns to save space\n",
    "cols_to_drop = ['billing_class', 'drug_unit_of_measurement', 'drug_type_of_measurement', \n",
    "                'modifiers', 'standard_charge_algorithm', 'additional_generic_notes', 'methodology']\n",
    "all_matches = all_matches.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "# Now we can save to json\n",
    "all_matches.to_json(unc_rex_json_path, orient='records', lines=True)\n",
    "\n",
    "# Lets validate the json file \n",
    "validator = ValidateJSON(unc_rex_json_path)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del unc_rex_df\n",
    "del unc_rex_transformed\n",
    "del all_matches\n",
    "del validator\n",
    "del unc_rex_csv_path\n",
    "del unc_rex_json_path\n",
    "\n",
    "print(\"UNC Rex Hospital test processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2fd93e",
   "metadata": {},
   "source": [
    "***\n",
    "## South Carolina Hospitals\n",
    "\n",
    "**OH BOY AM I PISSED ALREADY**\n",
    "\n",
    "#### What’s Happening\n",
    "\n",
    "* **APC codes (`code|1` with type = APC):**\n",
    "  These rows have payer names, plan names, and estimated amounts. That’s why they’re the only rows showing price info. APC = Ambulatory Payment Classification, a CMS grouping for outpatient procedures.\n",
    "\n",
    "* **HCPCS/CPT codes (in `code|2`, `code|3`, etc. with type = CPT/HCPCS):**\n",
    "  These rows often have no payer, plan, or estimated amount attached. Instead, they are mapped *into* the APC buckets, which then carry the pricing/plan info.\n",
    "\n",
    "* In other words: the hospital publishes payer-specific negotiated rates only at the APC level, while keeping CPT/HCPCS rows as “mappings” without dollar amounts.\n",
    "\n",
    "#### How We need to Work Around It\n",
    "\n",
    "1. **Build a crosswalk (mapping):**\n",
    "\n",
    "   * Use the `description` and `code|n` columns to connect CPT/HCPCS rows to their parent APC row (same description or grouping).\n",
    "   * Then join those CPT/HCPCS codes to the APC rows that actually carry pricing.\n",
    "     → This gives you a lookup where searching by CPT/HCPCS leads you to the APC (and thus the estimated amounts and plan names).\n",
    "\n",
    "2. **Validate mapping:**\n",
    "\n",
    "   * In practice, hospitals often list the CPT/HCPCS that roll up into each APC.\n",
    "   * You’ll need to check whether identical descriptions (e.g., “Inj, aflibercept hd, 1 mg”) appear across APC-coded and CPT-coded rows, and merge them.\n",
    "\n",
    "3. **Practical solution in analysis:**\n",
    "\n",
    "   * Search by CPT -> Find matching description -> Get its APC -> Pull plan names and estimated amounts from that APC row.\n",
    "\n",
    "#### On Legality\n",
    "\n",
    "\n",
    "  Not necessarily illegal. CMS’s **price transparency rule (2021–)** requires hospitals to publish:\n",
    "\n",
    "  * Gross charges\n",
    "  * Discounted cash prices\n",
    "  * Payer-specific negotiated charges\n",
    "  * De-identified min/max negotiated charges\n",
    "  * For at least 300 shoppable services (including CPT/HCPCS).\n",
    "\n",
    "Many hospitals comply only at the APC level (grouping multiple CPTs). This practice has been criticized as undermining the intent of transparency, but hospitals often argue it’s compliant because APCs are “billing codes.” Enforcement has been light, though CMS has fined some hospitals for noncompliance.\n",
    "\n",
    "## Addendums\n",
    "\n",
    "Link to addendums for crosswalking https://www.cms.gov/medicare/payment/prospective-payment-systems/hospital-outpatient-pps/quarterly-addenda-updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71878598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 1711 / 2015 rows.\n",
      "service_config.json UPDATED (1 bundle(s) changed). Backup saved.\n",
      "MUSC Health test processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# --------------- MUSC HEALTH  ---------------- ADDENDUM B NEEDED\n",
    "# ======================================================================\n",
    "\n",
    "import re\n",
    "from scripts.cleaners import standardize_payer_name\n",
    "from scripts.cleaners import apply_payer_standardization_to_json\n",
    "from scripts.bundle_validation import ValidateJSON\n",
    "from scripts.merge_cpt_to_apc import map_prices_to_hcpcs, load_addendum_b\n",
    "\n",
    "# Grab row for MUSC Health in Charleston, SC\n",
    "hos_name = 'MUSC Health'\n",
    "matching_hospitals = hospitals_df[hospitals_df['name'] == hos_name]\n",
    "if not matching_hospitals.empty:\n",
    "    musc_row = matching_hospitals.iloc[0]\n",
    "else:\n",
    "    print(f\"Hospital '{hos_name}' not found in the dataset\")\n",
    "    musc_row = None\n",
    "\n",
    "# grab json path for MUSC Health\n",
    "musc_json_path = musc_row['json_path']\n",
    "\n",
    "# load a single csv file from csv_folder for testing\n",
    "musc_csv_path = os.path.join(csv_folder, 'MUSC_Health_Medical_Center_CM.csv')\n",
    "musc_df = pd.read_csv(musc_csv_path)\n",
    "\n",
    "# Change all mixed type columns to string to avoid dtype issues (focus codes columns only)\n",
    "code_cols = [c for c in musc_df.columns if c.startswith(\"code|\") or c.startswith(\"code_\")]\n",
    "musc_df[code_cols] = musc_df[code_cols].astype(str)\n",
    "\n",
    "# ======================================================\n",
    "# ADDENDUM B LOADING\n",
    "# ======================================================\n",
    "\n",
    "# load addendum b for mapping, save folder as csv files are stored locally outside of CLEAR repo\n",
    "addendum_b_path = os.path.join(csv_folder, '2025_Web_Addendum_B.csv')\n",
    "\n",
    "# load addendum b\n",
    "addendum_b = load_addendum_b(addendum_b_path)\n",
    "\n",
    "# Map prices to hcpcs codes in musc_df\n",
    "musc_df = map_prices_to_hcpcs(musc_df, addendum_b, expand=True)\n",
    "\n",
    "# replace column name instances with | to _, code|1 becomes code_1, etc\n",
    "musc_df.columns = [re.sub(r'\\|', '_', col) for col in musc_df.columns]\n",
    "\n",
    "# Code structure is similar to Duke with there being multiple code columns\n",
    "\"\"\"\n",
    "Checking matches for column: code_1\n",
    "  HCPCS matches: 0\n",
    "  CPT matches: 0\n",
    "  Lab matches: 0\n",
    "Checking matches for column: code_2\n",
    "  HCPCS matches: 0\n",
    "  CPT matches: 10\n",
    "  Lab matches: 0\n",
    "Checking matches for column: code_3\n",
    "  HCPCS matches: 8\n",
    "  CPT matches: 0\n",
    "  Lab matches: 323\n",
    "Checking matches for column: code_4\n",
    "  HCPCS matches: 0\n",
    "  CPT matches: 0\n",
    "  Lab matches: 0\n",
    "\n",
    "Unique values in code_1_type: ['APC' 'CDM' 'MS-DRG' 'NDC']\n",
    "Unique values in code_2_type: ['nan' 'RC']\n",
    "Unique values in code_3_type: ['nan' 'HCPCS']\n",
    "Unique values in code_4_type: ['nan' 'NDC']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# now grab matches\n",
    "hcpcs_matches = musc_df[musc_df['code_3'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = musc_df[musc_df['code_2'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = musc_df[musc_df['code_3'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]  \n",
    "\n",
    "all_matches = pd.concat(match_dfs, ignore_index=True)\n",
    "\n",
    "# drop all rows where payer_name is null/empty, for now dont worry about est. price, drop duplicates after\n",
    "all_matches = all_matches[(all_matches['payer_name'].notna() & (all_matches['payer_name'] != ''))]\n",
    "all_matches = all_matches.drop_duplicates()\n",
    "\n",
    "# due to the mapping code types are a bit trickier now, espeically to keep records unique, for \n",
    "# now lets just use code_3/code_3_type if present, else code_2/code_2_type for code/type columns\n",
    "def select_code(row):\n",
    "    if pd.notna(row['code_3']) and row['code_3'] != '':\n",
    "        return pd.Series({'code': row['code_3'], 'type': row['code_3_type']})\n",
    "    elif pd.notna(row['code_2']) and row['code_2'] != '':\n",
    "        return pd.Series({'code': row['code_2'], 'type': row['code_2_type']})\n",
    "    else:\n",
    "        return pd.Series({'code': None, 'type': None})\n",
    "    \n",
    "if not all_matches.empty:\n",
    "    all_matches[['code', 'type']] = all_matches.apply(select_code, axis=1)\n",
    "    all_matches = all_matches.drop(columns=['code_2', 'code_2_type', 'code_3', 'code_3_type'])\n",
    "\n",
    "# now lets apply payer standardization\n",
    "all_matches['payer_name'] = all_matches['payer_name'].apply(standardize_payer_name)\n",
    "\n",
    "# Now lets remove enteries that don't have est. prices\n",
    "all_matches = all_matches[all_matches['estimated_amount'].notna() & (all_matches['estimated_amount'] != '')]\n",
    "\n",
    "# finally export to json\n",
    "all_matches.to_json(musc_json_path, orient='records', lines=True)\n",
    "\n",
    "# Validate the final json\n",
    "validator = ValidateJSON(musc_json_path)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del musc_df\n",
    "del musc_row\n",
    "del musc_csv_path\n",
    "del all_matches\n",
    "\n",
    "print(\"MUSC Health test processing complete.\")\n",
    "\n",
    "# ======================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
