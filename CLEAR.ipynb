{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a97bcb8",
   "metadata": {},
   "source": [
    "# CLEAR\n",
    "\n",
    "Chargemaster Location-based Exploration for Affordability & Reform\n",
    "\n",
    "This is the notebook file primarily responsible for pre-processing data, attaching important information, and generating database files for the github page. Below you can find all information about how data is processed from the downloaded `.csv` files found on most hospital sites. This is an exploratory project focused on creating interactive visualzations and tools to better inform people about their healthcare. The repo can always be maintained by downloading the most current year data for the specific hospital and putting it through the scripts. It should be noted that this is NOT a comprehensive list, but it can potentially be scaled to a full working-standalone site with enough time. \n",
    "\n",
    "All pre-processing code is written in python. See the `.html` files for how the D3 visualizations work. \n",
    "\n",
    "## How it works (Copied from README)\n",
    "\n",
    "Hospitals that have been added to this 'web-app' are stored in a `.csv` file for quick look up and ease of access. This points to the loc of it's Charge Master `.json` file which is then queried for the specific procedure. Hospitals are gathered from the CSV list based on a radius look-up provided by the user. If a hospital in the radius does not offer the service, it will not display the price point compared to others in the radius. \n",
    "\n",
    "Currently limited to 500 procedures due to file size limits and me not wanted to set up a database for this. Parquet only works server side so i can't do iterative testing before publishing to pages, and pages deployments can take a while.  \n",
    "\n",
    "## List of Hospitals\n",
    "\n",
    "These are the hospital's which data has been gathered and processed for thus far:\n",
    "\n",
    "| State    | Hospital Name                     | Zipcode     | Date                 | File Size    | Link                                                            |\n",
    "|----------|--------------------------------|-------------|-------------------|-------------|------------------------------------------------|\n",
    "| NC        | Duke University Hospital     |     27710    |      09/2025      |   3.32 GB   |                                                                   |\n",
    "| NC        | Wake Med                           |                   |                          |                   |                                                                 |\n",
    "| NC        | REX UNC                             |                   |                          |                   |                                                                 |\n",
    "\n",
    "## Outside Sources Used\n",
    "\n",
    "- zip_centroids.csv courtesy of SimpleMaps data https://simplemaps.com/data/us-zips.\n",
    "- CMS.gov data for top 200 HCPCS and CPT codes billed for 2024 & top 100 lab codes. [Link](https://www.cms.gov/data-research/statistics-trends-and-reports/medicare-fee-for-service-parts-a-b/medicare-utilization-part-b)\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bdabc1",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "CSV files are too large to store on github, thus they are downloaded locally, converted to the necessary format, then uploaded. If you want to perform conversions yourself you will need to find the specific hospital chargemaster and document in the notebook accordingly.\n",
    "\n",
    "Not all Charge Masters (CM) are formatted the same, as such, to keep this notebook from growing too large, custom python scripts will be made for unique CM's. This matters beccause some hospitals are regional or statewide 'chains' but can vary prices between locations. For example, \n",
    "\n",
    "**AdventHealth**\n",
    "- AdventHealth Orlando\n",
    "- AdventHealth Tampa\n",
    "- AdventHealth Hendersonville\n",
    "\n",
    "all are AdventHealth hospitals, but their prices and available procedures vary per location. However, the same script to clean and process their CM's works because the file structure doesn't change from loc to loc. Normally CM structure only changes from hospital to hospital (brand-wise), but I haven't looked at the majority of US hospitals so this statement might need to be amended. \n",
    "\n",
    "Think of this file as more of a \"**Controller**\" for the cleaning, while the cleaning process is performed by imported functions. Subsections from here on are labeled by State, be sure to check which Hospitals are in each subsection before uploading data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bf0c7",
   "metadata": {},
   "source": [
    "***\n",
    "## Payer & Plan Names\n",
    "\n",
    "Naming conventions for payer/plans differ across hospitals, making this a pain. Like is an exhaustive regex section to hopefully simplify this so that the functionality of the .html page remains. \n",
    "\n",
    "Idk where this fits in, I'll add it later to documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2519a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AdventHealth_Hendersonville_CM.csv. Dropping dataframe from memory...\n",
      "Processed AdventHealth_Orlando_CM.csv. Dropping dataframe from memory...\n",
      "Processed AdventHealth_Tampa_CM.csv. Dropping dataframe from memory...\n",
      "Processed DukeHospital_Durham.csv. Dropping dataframe from memory...\n",
      "Unique payer and plan names extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "# imports necessary for data regex searching and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# csv's are stored locally outside of CLEAR repo\n",
    "# set up one folder then into 'ChargeMaster_Project/csv_files/'\n",
    "# get path to csv_files folder outside CLEAR repo\n",
    "workspace_root = os.path.dirname(os.path.abspath('CLEAR.ipynb'))\n",
    "csv_folder = os.path.join(workspace_root, '..', 'ChargeMaster_Project', 'csv_files')\n",
    "csv_folder = os.path.abspath(csv_folder)\n",
    "\n",
    "# lets load all CM files into pandas dataframes, grab the unique values for 'payer_name' and 'plan_name' columns only\n",
    "# then drop everything else to save space\n",
    "for file in os.listdir(csv_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_folder, file)\n",
    "        df = pd.read_csv(file_path, dtype=str)  # read all columns as strings to avoid dtype issues\n",
    "        if 'payer_name' in df.columns and 'plan_name' in df.columns:\n",
    "            unique_payers = df['payer_name'].dropna().unique()\n",
    "            unique_plans = df['plan_name'].dropna().unique()\n",
    "            # save unique payers and plans to text files for later use\n",
    "            with open(os.path.join(csv_folder, f'{file}_unique_payers.txt'), 'w') as f:\n",
    "                for payer in unique_payers:\n",
    "                    f.write(f\"{payer}\\n\")\n",
    "            with open(os.path.join(csv_folder, f'{file}_unique_plans.txt'), 'w') as f:\n",
    "                for plan in unique_plans:\n",
    "                    f.write(f\"{plan}\\n\")\n",
    "        del df  # drop dataframe to save memory\n",
    "        print(f\"Processed {file}. Dropping dataframe from memory...\")\n",
    "print(\"Unique payer and plan names extracted and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load all the text files from the same csv_files folder for each hospital\n",
    "# and combine them into one master list of unique payers and plans\n",
    "all_unique_payers = set()\n",
    "all_unique_plans = set()\n",
    "for file in os.listdir(csv_folder):\n",
    "    if file.endswith('_unique_payers.txt'):\n",
    "        with open(os.path.join(csv_folder, file), 'r') as f:\n",
    "            payers = f.read().splitlines()\n",
    "            all_unique_payers.update(payers)\n",
    "    elif file.endswith('_unique_plans.txt'):\n",
    "        with open(os.path.join(csv_folder, file), 'r') as f:\n",
    "            plans = f.read().splitlines()\n",
    "            all_unique_plans.update(plans)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cfa7bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing enhanced payer name standardization:\n",
      "------------------------------------------------------------\n",
      "Blue_Cross_&_Blue_Shield_of_Florida           -> BLUE CROSS BLUE SHIELD OF FLORIDA\n",
      "Blue_Cross_&_Blue_Shield_of_Florida_          -> BLUE CROSS BLUE SHIELD OF FLORIDA\n",
      "Cigna_HealthCare                              -> CIGNA\n",
      "Longevity                                     -> LONGEVITY\n",
      "Aetna_Health                                  -> AETNA\n",
      "CIGNA [1107150]                               -> CIGNA\n",
      "UHC                                           -> UNITED HEALTHCARE\n",
      "AETNA [1107164]                               -> AETNA\n",
      "UHC [1107151]                                 -> UNITED HEALTHCARE\n",
      "NALC HEALTH BENEFIT PLAN [1001268]            -> NALC HEALTH BENEFIT PLAN\n",
      "DUKE PLUS                                     -> DUKE PLUS\n",
      "MAIL HANDLERS [1001414]                       -> MAIL HANDLERS\n",
      "AETNA                                         -> AETNA\n",
      "FIRST HEALTH [1107113]                        -> FIRST HEALTH\n",
      "GOLDEN RULE INSURANCE COMPANY [1001209]       -> GOLDEN RULE INSURANCE\n",
      "OXFORD HEALTH PLANS [1001285]                 -> OXFORD HEALTH PLANS\n",
      "UNITED MEDICAL RESOURCES CONTRACT [1107140]   -> UNITED HEALTHCARE\n",
      "UMR [1107154]                                 -> UNITED HEALTHCARE\n",
      "CIGNA                                         -> CIGNA\n",
      "Humana Inc                                    -> HUMANA\n",
      "HUMANA [123456]                               -> HUMANA\n",
      "Kaiser Permanente                             -> KAISER PERMANENTE PERMANENTE\n",
      "KAISER [789012]                               -> KAISER PERMANENTE\n",
      "Anthem Blue Cross                             -> ANTHEM BLUE CROSS\n",
      "ANTHEM [345678]                               -> ANTHEM\n",
      "Wellcare                                      -> WELLCARE\n",
      "Well Care Health Plans                        -> WELLCARE HEALTH PLANS\n",
      "Molina Healthcare                             -> MOLINA HEALTHCARE HEALTHCARE\n",
      "MOLINA [901234]                               -> MOLINA HEALTHCARE\n",
      "BCBS OF NORTH CAROLINA                        -> BLUE CROSS BLUE SHIELD OF NORTH CAROLINA\n",
      "BCBS                                          -> BLUE CROSS BLUE SHIELD\n",
      "Medicare                                      -> MEDICARE\n",
      "MEDICARE [567890]                             -> MEDICARE\n",
      "Medicaid                                      -> MEDICAID\n",
      "TRICARE                                       -> TRICARE\n",
      "Tri-Care                                      -> TRICARE\n",
      "Workers Comp                                  -> WORKERS COMPENSATION\n",
      "WORKERS COMPENSATION                          -> WORKERS COMPENSATION\n",
      "WC                                            -> WORKERS COMPENSATION\n",
      "Auto Insurance                                -> AUTO INSURANCE\n",
      "Motor Vehicle                                 -> AUTO INSURANCE\n",
      "PIP                                           -> AUTO INSURANCE PIP\n",
      "Self Pay                                      -> SELF PAY\n",
      "CASH                                          -> SELF PAY\n",
      "Self Insured                                  -> SELF PAY\n",
      "FEHB                                          -> FEDERAL EMPLOYEE HEALTH BENEFITS\n",
      "Federal Employee Health Benefits              -> FEDERAL EMPLOYEE HEALTH BENEFITS\n",
      "GEHA                                          -> GOVERNMENT EMPLOYEES HEALTH ASSOCIATION\n",
      "Health Net                                    -> HEALTH NET\n",
      "AMBETTER                                      -> AMBETTER\n",
      "United of Omaha                               -> UNITED OF OMAHA\n",
      "\n",
      "============================================================\n",
      "Standardization function ready for use on your CSV files!\n",
      "Use apply_payer_standardization_to_csv(filepath) to apply to actual data.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Payer name standardization function\n",
    "def standardize_payer_name(payer_name):\n",
    "    \"\"\"\n",
    "    Standardize payer names while preserving important distinctions like state-specific plans\n",
    "    \"\"\"\n",
    "    if pd.isna(payer_name) or payer_name == '':\n",
    "        return payer_name\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    name = str(payer_name).strip()\n",
    "    \n",
    "    # Remove trailing underscores and extra spaces\n",
    "    name = re.sub(r'_+$', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    \n",
    "    # Standardize common insurance company names while preserving state distinctions\n",
    "    standardization_patterns = [\n",
    "        # Aetna variations\n",
    "        (r'\\bAETNA\\b.*?(?:\\[[\\d]+\\])?', 'AETNA'),\n",
    "        (r'\\bAetna.*?Health\\b', 'AETNA'),\n",
    "        (r'\\bAetna.*?Better.*?Health\\b', 'AETNA'),\n",
    "        \n",
    "        # Cigna variations\n",
    "        (r'\\bCIGNA\\b.*?(?:\\[[\\d]+\\])?', 'CIGNA'),\n",
    "        (r'\\bCigna.*?HealthCare\\b', 'CIGNA'),\n",
    "        (r'\\bCigna.*?Health.*?Care\\b', 'CIGNA'),\n",
    "        \n",
    "        # UHC/United variations\n",
    "        (r'\\bUHC\\b.*?(?:\\[[\\d]+\\])?', 'UNITED HEALTHCARE'),\n",
    "        (r'\\bUNITED\\s+HEALTHCARE\\b', 'UNITED HEALTHCARE'),\n",
    "        (r'\\bUNITED\\s+HEALTH\\s+GROUP\\b', 'UNITED HEALTHCARE'),\n",
    "        (r'\\bUNITED\\s+MEDICAL\\s+RESOURCES.*CONTRACT\\b', 'UNITED HEALTHCARE'),\n",
    "        (r'\\bUMR\\b.*?(?:\\[[\\d]+\\])?', 'UNITED HEALTHCARE'),\n",
    "        (r'\\bUNITED\\s+OF\\s+OMAHA\\b', 'UNITED OF OMAHA'),  # Different company\n",
    "        \n",
    "        # Humana variations\n",
    "        (r'\\bHUMANA\\b.*?(?:\\[[\\d]+\\])?', 'HUMANA'),\n",
    "        (r'\\bHumana.*?Inc\\b', 'HUMANA'),\n",
    "        \n",
    "        # Anthem/BCBS Anthem variations\n",
    "        (r'\\bANTHEM\\b.*?(?:\\[[\\d]+\\])?', 'ANTHEM'),\n",
    "        (r'\\bAnthem.*?Blue.*?Cross\\b', 'ANTHEM BLUE CROSS'),\n",
    "        \n",
    "        # Kaiser variations\n",
    "        (r'\\bKAISER\\b.*?(?:\\[[\\d]+\\])?', 'KAISER PERMANENTE'),\n",
    "        (r'\\bKaiser.*?Permanente\\b', 'KAISER PERMANENTE'),\n",
    "        \n",
    "        # Wellcare variations\n",
    "        (r'\\bWELLCARE\\b.*?(?:\\[[\\d]+\\])?', 'WELLCARE'),\n",
    "        (r'\\bWell.*?Care\\b', 'WELLCARE'),\n",
    "        \n",
    "        # Molina variations\n",
    "        (r'\\bMOLINA\\b.*?(?:\\[[\\d]+\\])?', 'MOLINA HEALTHCARE'),\n",
    "        (r'\\bMolina.*?Healthcare\\b', 'MOLINA HEALTHCARE'),\n",
    "        \n",
    "        # Blue Cross Blue Shield - preserve state distinctions\n",
    "        (r'\\bBlue_Cross_&_Blue_Shield_of_([A-Za-z_]+)_?', r'BLUE CROSS BLUE SHIELD OF \\1'),\n",
    "        (r'\\bBLUE\\s+CROSS\\s+BLUE\\s+SHIELD\\s+OF\\s+([A-Z\\s]+)', r'BLUE CROSS BLUE SHIELD OF \\1'),\n",
    "        (r'\\bBCBS\\s+OF\\s+([A-Z\\s]+)', r'BLUE CROSS BLUE SHIELD OF \\1'),\n",
    "        (r'\\bBCBS\\b', 'BLUE CROSS BLUE SHIELD'),\n",
    "        \n",
    "        # Medicare/Medicaid variations\n",
    "        (r'\\bMEDICARE\\b.*?(?:\\[[\\d]+\\])?', 'MEDICARE'),\n",
    "        (r'\\bMEDICAID\\b.*?(?:\\[[\\d]+\\])?', 'MEDICAID'),\n",
    "        (r'\\bCMS\\b.*?(?:\\[[\\d]+\\])?', 'MEDICARE'),\n",
    "        \n",
    "        # Tricare variations\n",
    "        (r'\\bTRICARE\\b.*?(?:\\[[\\d]+\\])?', 'TRICARE'),\n",
    "        (r'\\bTRI.*?CARE\\b', 'TRICARE'),\n",
    "        \n",
    "        # Workers Compensation variations\n",
    "        (r'\\bWORKERS.*?COMP\\b', 'WORKERS COMPENSATION'),\n",
    "        (r'\\bWORKERS.*?COMPENSATION\\b', 'WORKERS COMPENSATION'),\n",
    "        (r'\\bWC\\b(?!\\s+\\d)', 'WORKERS COMPENSATION'),  # Not followed by numbers\n",
    "        \n",
    "        # Auto Insurance variations\n",
    "        (r'\\bAUTO\\s+INSURANCE\\b', 'AUTO INSURANCE'),\n",
    "        (r'\\bMOTOR\\s+VEHICLE\\b', 'AUTO INSURANCE'),\n",
    "        (r'\\bPIP\\b(?!\\s+\\d)', 'AUTO INSURANCE PIP'),\n",
    "        \n",
    "        # Self Pay variations\n",
    "        (r'\\bSELF.*?PAY\\b', 'SELF PAY'),\n",
    "        (r'\\bCASH\\b(?!\\s+\\d)', 'SELF PAY'),\n",
    "        (r'\\bSELF.*?INSURED\\b', 'SELF PAY'),\n",
    "        \n",
    "        # Remove ID numbers in brackets at the end\n",
    "        (r'\\s*\\[[\\d]+\\]\\s*$', ''),\n",
    "        \n",
    "        # Standardize specific plans and smaller insurers\n",
    "        (r'\\bDUKE\\s+PLUS\\b', 'DUKE PLUS'),\n",
    "        (r'\\bMAIL\\s+HANDLERS\\b.*?(?:\\[[\\d]+\\])?', 'MAIL HANDLERS'),\n",
    "        (r'\\bNALC\\s+HEALTH\\s+BENEFIT\\s+PLAN\\b.*?(?:\\[[\\d]+\\])?', 'NALC HEALTH BENEFIT PLAN'),\n",
    "        (r'\\bFIRST\\s+HEALTH\\b.*?(?:\\[[\\d]+\\])?', 'FIRST HEALTH'),\n",
    "        (r'\\bGOLDEN\\s+RULE\\s+INSURANCE\\s+COMPANY\\b.*?(?:\\[[\\d]+\\])?', 'GOLDEN RULE INSURANCE'),\n",
    "        (r'\\bOXFORD\\s+HEALTH\\s+PLANS\\b.*?(?:\\[[\\d]+\\])?', 'OXFORD HEALTH PLANS'),\n",
    "        (r'\\bHEALTH\\s+NET\\b.*?(?:\\[[\\d]+\\])?', 'HEALTH NET'),\n",
    "        (r'\\bAMBETTER\\b.*?(?:\\[[\\d]+\\])?', 'AMBETTER'),\n",
    "        (r'\\bCENTENE\\b.*?(?:\\[[\\d]+\\])?', 'CENTENE'),\n",
    "        \n",
    "        # Federal Employee plans\n",
    "        (r'\\bFEHB\\b', 'FEDERAL EMPLOYEE HEALTH BENEFITS'),\n",
    "        (r'\\bFEDERAL\\s+EMPLOYEE.*?HEALTH.*?BENEFITS\\b', 'FEDERAL EMPLOYEE HEALTH BENEFITS'),\n",
    "        (r'\\bGEHA\\b', 'GOVERNMENT EMPLOYEES HEALTH ASSOCIATION'),\n",
    "        \n",
    "        # Convert underscores to spaces for better readability\n",
    "        (r'_', ' '),\n",
    "        \n",
    "        # Clean up multiple spaces\n",
    "        (r'\\s+', ' '),\n",
    "        \n",
    "        # Fix common OCR/data entry errors\n",
    "        (r'\\b0\\b', 'O'),  # Replace standalone 0 with O\n",
    "        (r'\\bl\\b', 'I'),  # Replace standalone l with I\n",
    "    ]\n",
    "    \n",
    "    # Apply standardization patterns\n",
    "    for pattern, replacement in standardization_patterns:\n",
    "        name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n",
    "    \n",
    "    return name.strip().upper()\n",
    "\n",
    "# Enhanced test with more examples including common healthcare payers\n",
    "test_payers = [\n",
    "    # Original examples\n",
    "    'Blue_Cross_&_Blue_Shield_of_Florida',\n",
    "    'Blue_Cross_&_Blue_Shield_of_Florida_',\n",
    "    'Cigna_HealthCare',\n",
    "    'Longevity',\n",
    "    'Aetna_Health',\n",
    "    'CIGNA [1107150]',\n",
    "    'UHC',\n",
    "    'AETNA [1107164]',\n",
    "    'UHC [1107151]',\n",
    "    'NALC HEALTH BENEFIT PLAN [1001268]',\n",
    "    'DUKE PLUS',\n",
    "    'MAIL HANDLERS [1001414]',\n",
    "    'AETNA',\n",
    "    'FIRST HEALTH [1107113]',\n",
    "    'GOLDEN RULE INSURANCE COMPANY [1001209]',\n",
    "    'OXFORD HEALTH PLANS [1001285]',\n",
    "    'UNITED MEDICAL RESOURCES CONTRACT [1107140]',\n",
    "    'UMR [1107154]',\n",
    "    'CIGNA',\n",
    "    \n",
    "    # Additional common variations\n",
    "    'Humana Inc',\n",
    "    'HUMANA [123456]',\n",
    "    'Kaiser Permanente',\n",
    "    'KAISER [789012]',\n",
    "    'Anthem Blue Cross',\n",
    "    'ANTHEM [345678]',\n",
    "    'Wellcare',\n",
    "    'Well Care Health Plans',\n",
    "    'Molina Healthcare',\n",
    "    'MOLINA [901234]',\n",
    "    'BCBS OF NORTH CAROLINA',\n",
    "    'BCBS',\n",
    "    'Medicare',\n",
    "    'MEDICARE [567890]',\n",
    "    'Medicaid',\n",
    "    'TRICARE',\n",
    "    'Tri-Care',\n",
    "    'Workers Comp',\n",
    "    'WORKERS COMPENSATION',\n",
    "    'WC',\n",
    "    'Auto Insurance',\n",
    "    'Motor Vehicle',\n",
    "    'PIP',\n",
    "    'Self Pay',\n",
    "    'CASH',\n",
    "    'Self Insured',\n",
    "    'FEHB',\n",
    "    'Federal Employee Health Benefits',\n",
    "    'GEHA',\n",
    "    'Health Net',\n",
    "    'AMBETTER',\n",
    "    'United of Omaha'\n",
    "]\n",
    "\n",
    "print(\"Testing enhanced payer name standardization:\")\n",
    "print(\"-\" * 60)\n",
    "for payer in test_payers:\n",
    "    standardized = standardize_payer_name(payer)\n",
    "    print(f\"{payer:<45} -> {standardized}\")\n",
    "\n",
    "# Also create a function to apply standardization to actual data\n",
    "def apply_payer_standardization_to_csv(csv_file_path):\n",
    "    \"\"\"\n",
    "    Apply payer standardization to a CSV file and return the modified dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file_path, dtype=str)\n",
    "    \n",
    "    if 'payer_name' in df.columns:\n",
    "        print(f\"Standardizing payer names in {csv_file_path}\")\n",
    "        df['payer_name'] = df['payer_name'].apply(standardize_payer_name)\n",
    "        \n",
    "        # Show before/after unique counts\n",
    "        print(f\"Unique payer names after standardization: {df['payer_name'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Standardization function ready for use on your CSV files!\")\n",
    "print(\"Use apply_payer_standardization_to_csv(filepath) to apply to actual data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization to all existing CSV files and update the unique payers/plans files\n",
    "def update_all_csv_files_with_standardization():\n",
    "    \"\"\"\n",
    "    Apply payer name standardization to all CSV files in the csv_folder\n",
    "    and regenerate the unique payers/plans text files\n",
    "    \"\"\"\n",
    "    updated_files = []\n",
    "    \n",
    "    for file in os.listdir(csv_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(csv_folder, file)\n",
    "            \n",
    "            # Load the CSV\n",
    "            df = pd.read_csv(file_path, dtype=str)\n",
    "            \n",
    "            # Apply standardization if payer_name column exists\n",
    "            if 'payer_name' in df.columns:\n",
    "                original_unique_count = df['payer_name'].nunique()\n",
    "                df['payer_name'] = df['payer_name'].apply(standardize_payer_name)\n",
    "                new_unique_count = df['payer_name'].nunique()\n",
    "                \n",
    "                # Save the updated CSV\n",
    "                df.to_csv(file_path, index=False)\n",
    "                \n",
    "                # Regenerate unique payers file\n",
    "                unique_payers = df['payer_name'].dropna().unique()\n",
    "                with open(os.path.join(csv_folder, f'{file}_unique_payers.txt'), 'w') as f:\n",
    "                    for payer in sorted(unique_payers):  # Sort for easier reading\n",
    "                        f.write(f\"{payer}\\n\")\n",
    "                \n",
    "                # Also update unique plans if exists\n",
    "                if 'plan_name' in df.columns:\n",
    "                    unique_plans = df['plan_name'].dropna().unique()\n",
    "                    with open(os.path.join(csv_folder, f'{file}_unique_plans.txt'), 'w') as f:\n",
    "                        for plan in sorted(unique_plans):\n",
    "                            f.write(f\"{plan}\\n\")\n",
    "                \n",
    "                updated_files.append({\n",
    "                    'file': file,\n",
    "                    'original_payers': original_unique_count,\n",
    "                    'standardized_payers': new_unique_count,\n",
    "                    'reduction': original_unique_count - new_unique_count\n",
    "                })\n",
    "                \n",
    "                print(f\"Updated {file}: {original_unique_count} -> {new_unique_count} unique payers \"\n",
    "                      f\"({original_unique_count - new_unique_count} consolidated)\")\n",
    "    \n",
    "    return updated_files\n",
    "\n",
    "# Uncomment the line below to run the standardization on all your CSV files\n",
    "# results = update_all_csv_files_with_standardization()\n",
    "\n",
    "print(\"Standardization update function ready!\")\n",
    "print(\"Uncomment the last line to apply standardization to all CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ce11e",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Preloading Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5d6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospitals.csv updater/editor\n",
    "import hashlib\n",
    "import requests\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"CLEAR-geoapi-2025\")\n",
    "csv_file = 'docs/data/hospitals.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# construct address for geocoding only (don't modify original data)\n",
    "def construct_geocoding_address(row):\n",
    "    # Build clean address from original components\n",
    "    address = f\"{row['address']}, {row['city']}, {row['state']} {row['zip']}\"\n",
    "    return address\n",
    "\n",
    "# get lat/lon from address with increased timeout and retry/delay\n",
    "def get_lat_lon(address, max_retries=3, delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            location = geolocator.geocode(address, timeout=5)\n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "            else:\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding {address} (attempt {attempt+1}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None, None\n",
    "\n",
    "# generate short unique ID based on ['hospital'] + full composite address (base36, 8 chars)\n",
    "def generate_short_id(row):\n",
    "    full_address = construct_geocoding_address(row)\n",
    "    unique_string = f\"{row['name']}_{full_address}\"\n",
    "    hash_int = int(hashlib.md5(unique_string.encode()).hexdigest(), 16)\n",
    "    short_id = base36encode(hash_int)[:8]\n",
    "    return short_id\n",
    "\n",
    "# base36 encoding for shorter IDs\n",
    "def base36encode(number):\n",
    "    chars = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    if number == 0:\n",
    "        return '0'\n",
    "    result = ''\n",
    "    while number > 0:\n",
    "        number, i = divmod(number, 36)\n",
    "        result = chars[i] + result\n",
    "    return result\n",
    "\n",
    "# Add lat/lon and short_id to dataframe, set json_path to be '/data/prices/['state']/['id'].json'\n",
    "def update_dataframe(df):\n",
    "    \n",
    "    # Don't modify the address column - just use it for geocoding\n",
    "    def lat_lon_with_delay(row):\n",
    "        geocoding_address = construct_geocoding_address(row)\n",
    "        lat, lon = get_lat_lon(geocoding_address)\n",
    "        time.sleep(1)  # 1 second delay per request\n",
    "        return pd.Series([lat, lon])\n",
    "    \n",
    "    df[['lat', 'lon']] = df.apply(lat_lon_with_delay, axis=1)\n",
    "    df['id'] = df.apply(generate_short_id, axis=1)\n",
    "    df['json_path'] = df.apply(lambda row: f\"docs/data/prices/{row['state']}/{row['id']}.json\", axis=1)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "update_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d6068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank by Charges</th>\n",
       "      <th>HCPCS Code</th>\n",
       "      <th>Allowed Charges</th>\n",
       "      <th>Allowed Services</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99214</td>\n",
       "      <td>12,493,376,407</td>\n",
       "      <td>103,756,876</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>99213</td>\n",
       "      <td>5,914,372,895</td>\n",
       "      <td>69,301,624</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>99233</td>\n",
       "      <td>2,693,744,916</td>\n",
       "      <td>22,975,112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99232</td>\n",
       "      <td>2,676,454,801</td>\n",
       "      <td>34,687,153</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>99215</td>\n",
       "      <td>2,166,116,667</td>\n",
       "      <td>12,926,784</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank by Charges  HCPCS Code  Allowed Charges Allowed Services  Unnamed: 4\n",
       "0                1       99214  12,493,376,407      103,756,876          NaN\n",
       "1                2       99213   5,914,372,895       69,301,624          NaN\n",
       "2                3       99233   2,693,744,916       22,975,112          NaN\n",
       "3                4       99232   2,676,454,801       34,687,153          NaN\n",
       "4                5       99215   2,166,116,667       12,926,784          NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to create comparison df's for the top 200 HCPCS and CMS codes billed for 2024 & top 100 lab codes\n",
    "# first load the codes from the .csv files\n",
    "hcpcs_codes = pd.read_csv('docs/data/hcpcs_lvl2_top_200_codes_2024.csv')\n",
    "lab_codes = pd.read_csv('docs/data/lab_top_100_codes_2024.csv')\n",
    "cpt_codes = pd.read_csv('docs/data/cpt_lvl1_top_200_codes_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48eb048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TO LOAD HOSPITALS CSV\n",
    "import os\n",
    "\n",
    "# csv's are stored locally outside of CLEAR repo\n",
    "# set up one folder then into 'ChargeMaster_Project/csv_files/'\n",
    "# get path to csv_files folder outside CLEAR repo\n",
    "workspace_root = os.path.dirname(os.path.abspath('CLEAR.ipynb'))\n",
    "csv_folder = os.path.join(workspace_root, '..', 'ChargeMaster_Project', 'csv_files')\n",
    "csv_folder = os.path.abspath(csv_folder)\n",
    "\n",
    "# define path to hospitals.csv\n",
    "hospitals_csv = os.path.join(workspace_root, 'docs', 'data', 'hospitals.csv')\n",
    "hospitals_csv = os.path.abspath(hospitals_csv)\n",
    "\n",
    "# read hospitals.csv to get list of hospitals and their file paths\n",
    "hospitals_df = pd.read_csv(hospitals_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208356c9",
   "metadata": {},
   "source": [
    "***\n",
    "## North Carolina Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9ff6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcing\\AppData\\Local\\Temp\\ipykernel_19156\\362494677.py:19: DtypeWarning: Columns (7,8,9,12,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  duke_df = pd.read_csv(test_csv_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================================\n",
    "# --------------- DUKE HOSPITAL TESTING ----------------\n",
    "# ======================================================================\n",
    "\n",
    "# Grab row for Duke Hospital in Durham, NC\n",
    "hos_name = 'Duke University Hospital'\n",
    "matching_hospitals = hospitals_df[hospitals_df['name'] == hos_name]\n",
    "if not matching_hospitals.empty:\n",
    "    duke_row = matching_hospitals.iloc[0]\n",
    "else:\n",
    "    print(f\"Hospital '{hos_name}' not found in the dataset\")\n",
    "    duke_row = None\n",
    "\n",
    "# grab json path for Duke Hospital\n",
    "duke_json_path = duke_row['json_path']\n",
    "\n",
    "# load a single csv file from csv_folder for testing\n",
    "test_csv_path = os.path.join(csv_folder, 'DukeHospital_Durham.csv')\n",
    "duke_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# remove duke_df Hospital, City, State, Address columns before converting to parquet\n",
    "duke_df = duke_df.drop(columns=['Hospital', 'City', 'State', 'Address'])\n",
    "\n",
    "\n",
    "#code_cols = ['code_1', 'code_2', 'code_3', 'code_4']\n",
    "# Check matches for each code column against hcpcs_codes, cpt_codes, and lab_codes, iteratively\n",
    "# for col in code_cols:\n",
    "#     print(f\"Checking matches for column: {col}\")\n",
    "#     hcpcs_matches = duke_df[duke_df[col].isin(hcpcs_codes['HCPCS Code'])]\n",
    "#     cpt_matches = duke_df[duke_df[col].isin(cpt_codes['HCPCS Code'])]\n",
    "#     lab_matches = duke_df[duke_df[col].isin(lab_codes['HCPCS Code'])]\n",
    "#     print(f\"  HCPCS matches: {len(hcpcs_matches)}\")\n",
    "#     print(f\"  CPT matches: {len(cpt_matches)}\")\n",
    "#     print(f\"  Lab matches: {len(lab_matches)}\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    This actually shows that code_2 contains HCPCS codes and code_3 contains CPT codes\n",
    "    Checking matches for column: code_1\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 0\n",
    "        Lab matches: 0\n",
    "    Checking matches for column: code_2\n",
    "        HCPCS matches: 76966\n",
    "        CPT matches: 0\n",
    "        Lab matches: 19987\n",
    "    Checking matches for column: code_3\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 772\n",
    "        Lab matches: 0\n",
    "    Checking matches for column: code_4\n",
    "        HCPCS matches: 0\n",
    "        CPT matches: 0\n",
    "        Lab matches: 0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Duke Hospital CM Structure\n",
    "# code_2/code_3 [columns 3, 5 --> 4, 6 contain type] contain HCPCS and CPT codes, so we use those for comparison against the top 200 lists\n",
    "# Columns 13-24 contain payer, plan, and pricing info, so we want all of those as well as column 0 which is the \n",
    "# description of the code [used for regex matching on the front end]\n",
    "# final columns to keep: 0, 3-6, 13-24\n",
    "duke_df = duke_df.iloc[:, [0] + list(range(3, 7)) + list(range(13, 25))]\n",
    "\n",
    "# actually lets go ahead and drop some columns to conserve space\n",
    "duke_df = duke_df.drop(columns=['standard_charge_algorithm', 'additional_generic_notes'])\n",
    "\n",
    "# now we can search duke_df['code_2'] and duke_df['code_2_type'] against hcpcs_codes , cpt_codes, and lab_codes\n",
    "# first search hcpcs_codes\n",
    "hcpcs_matches = duke_df[duke_df['code_2'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = duke_df[duke_df['code_3'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = duke_df[duke_df['code_2'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "else:\n",
    "    # Create empty DataFrame with same structure as duke_df if no matches\n",
    "    all_matches = pd.DataFrame(columns=duke_df.columns)\n",
    "\n",
    "# There are some duplicate issues, mainly rows where no est. price are given, so lets remove enteries that don't have est. prices\n",
    "all_matches = all_matches[all_matches['estimated_amount'].notna() & (all_matches['estimated_amount'] != '')]\n",
    "\n",
    "# Save output data to json file for Duke json path\n",
    "all_matches.to_json(duke_json_path, orient='records', lines=True)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del duke_df\n",
    "del duke_row\n",
    "del test_csv_path\n",
    "\n",
    "# ======================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63e4f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcing\\AppData\\Local\\Temp\\ipykernel_19156\\3568986292.py:27: DtypeWarning: Columns (1,2,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  adv_nc_df = pd.read_csv(adv_nc_csv_path)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# --------------- ADVENTHEALTH HOSPITAL  ----------------\n",
    "# ======================================================================\n",
    "\n",
    "# Load AdventHealth Hendersonville, NC paths\n",
    "hos_name = 'AdventHealth'\n",
    "city_name = 'Hendersonville'\n",
    "state_name = 'NC'\n",
    "matching_hospitals = hospitals_df[\n",
    "    (hospitals_df['name'] == hos_name) &\n",
    "    (hospitals_df['state'] == state_name) &\n",
    "    (hospitals_df['city'] == city_name)\n",
    "]\n",
    "if not matching_hospitals.empty:\n",
    "    adv_nc = matching_hospitals.iloc[0]\n",
    "else:\n",
    "    print(f\"Hospital '{hos_name}' not found in the dataset\")\n",
    "    adv_nc = None\n",
    "\n",
    "# grab json path for AdventHealth Hendersonville, NC\n",
    "adv_nc_json_path = adv_nc['json_path']\n",
    "\n",
    "# load a single csv file from csv_folder for testing\n",
    "adv_nc_csv_path = os.path.join(csv_folder, 'AdventHealth_Hendersonville_CM.csv')\n",
    "\n",
    "# load AdventHealth Hendersonville, NC csv\n",
    "adv_nc_df = pd.read_csv(adv_nc_csv_path)\n",
    "\n",
    "# AdventHealth CM Structure\n",
    "# ['description', 'drug_information', 'code', 'type',\n",
    "#    'standard_charge_min', 'standard_charge_max', 'gross_charge',\n",
    "#    'discounted_cash', 'setting', 'payer_name', 'plan_name',\n",
    "#    'standard_charge_dollar', 'standard_charge_percentage',\n",
    "#    'estimated_amount', 'methodology', 'standard_charge_algorithm',\n",
    "#    'Hospital', 'City', 'State', 'Address']\n",
    "\n",
    "# Check matches for code column against hcpcs_codes, cpt_codes, and lab_codes\n",
    "# Output: \n",
    "# HCPCS matches: 20845\n",
    "#   CPT matches: 2884\n",
    "#   Lab matches: 13555\n",
    "\n",
    "# Lets drop unneeded columns, and rename some before grabing the matches and saving to json\n",
    "# NOTE: common naming convention needs to be added before renaming cols\n",
    "cols_to_drop = ['methodology', 'drug_information', 'standard_charge_algorithm', 'Hospital', 'City', 'State', 'Address']\n",
    "adv_nc_df = adv_nc_df.drop(columns=cols_to_drop)\n",
    "\n",
    "# now grab matches\n",
    "hcpcs_matches = adv_nc_df[adv_nc_df['code'].isin(hcpcs_codes['HCPCS Code'])]\n",
    "cpt_matches = adv_nc_df[adv_nc_df['code'].isin(cpt_codes['HCPCS Code'])]\n",
    "lab_matches = adv_nc_df[adv_nc_df['code'].isin(lab_codes['HCPCS Code'])]\n",
    "\n",
    "# Combine all matches into one dataframe, drop duplicates\n",
    "match_dfs = [df for df in [hcpcs_matches, cpt_matches, lab_matches] if not df.empty]\n",
    "\n",
    "if match_dfs:\n",
    "    all_matches = pd.concat(match_dfs, ignore_index=True).drop_duplicates()\n",
    "else:\n",
    "    # Create empty DataFrame with same structure as duke_df if no matches\n",
    "    all_matches = pd.DataFrame(columns=duke_df.columns)\n",
    "\n",
    "# There are some duplicate issues, mainly rows where no est. price are given, so lets remove enteries that don't have est. prices\n",
    "all_matches = all_matches[all_matches['estimated_amount'].notna() & (all_matches['estimated_amount'] != '')]\n",
    "\n",
    "# Save output data to json file for AdventHealth Hendersonville, NC json path\n",
    "all_matches.to_json(adv_nc_json_path, orient='records', lines=True)\n",
    "\n",
    "# drop file/df from memory to save space\n",
    "del adv_nc_df\n",
    "del adv_nc\n",
    "del adv_nc_csv_path\n",
    "\n",
    "# ======================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
